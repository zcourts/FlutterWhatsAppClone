"""This directive allows results to be deferred during execution"""
directive @defer on FIELD

directive @indices(sets: [[String]]) on OBJECT | INTERFACE

directive @computed(query: String, type: String, limit: Int, postQueryFn: String) on FIELD | FIELD_DEFINITION

"""check if the date is in the past"""
directive @past on FIELD_DEFINITION

"""check if the date is in the future"""
directive @future on FIELD_DEFINITION

"""Added by Hypi to objects and fields it generates"""
directive @generated on SCALAR | OBJECT | FIELD_DEFINITION | UNION | ENUM | INPUT_OBJECT

"""
Tells Hypi not to generate a table internally for the type this is attached to.
"""
directive @notable on OBJECT

"""Add to any field which should not have a relay API generated"""
directive @norelay on FIELD | FIELD_DEFINITION

"""If present on an object, no aggregation is possible on that type"""
directive @noagg on OBJECT

directive @computedCount(field: String, distinct: Boolean = false) on FIELD_DEFINITION

""" directive which allows any GraphQL definition's field to be provided by a Hypi Arc Tan function.
"""
directive @tan(type: TanType!, name: String, handler: String, inline: String, saveAs: String) on FIELD_DEFINITION

"""check if the value is not null"""
directive @notNull on FIELD_DEFINITION

"""
When applied causes the value of the field it is applied to to be resolved using an HTTP query configured with the given parameters
Each argument (url, body, query and headers) is a valid Velocity template).

The following may be referenced "vars", "settings"
vars refers to any arguments on the field the directive is applied to so ${vars.firstName} refers to the firstName arg of the field
settings refers to any instance settings provided in the app

Utilities:
JSON and Map utilities are available for use in all templates.
JsonNode JSON.parseJSON(String)
Map<String, Object> JSON.parse(String)

are available. Map is the standard Java Map interface containing static util methods e.g. Map.of(...)
"""
directive @http(method: HttpMethod! = GET, url: String!, headers: String, requestTemplate: String, inline: InlineHttpRequestTemplate, saveAs: String) on FIELD_DEFINITION

directive @embedded on FIELD_DEFINITION

"""
Indicates that the field cannot be changed after it is initially created
"""
directive @immutable on FIELD_DEFINITION

"""
Valid on String, Object and Array fields.
Check if the string or array length matches the range.
Checks if the object's list of non-null fields matches the range
"""
directive @length(min: Long, max: Long) on FIELD_DEFINITION

"""
check if the string or array is not null nor empty (for strings verifies its not all whitespace).
"""
directive @notEmpty on FIELD_DEFINITION

"""
Allows every type and function to be marked with the app and release where it was defined. This helps to identify where dependent types and functions came from in a schema
"""
directive @src(releaseId: String) on SCHEMA | SCALAR | OBJECT | FIELD_DEFINITION | INTERFACE | UNION | ENUM | INPUT_OBJECT

"""
Used internally to mark that a field in the schema should be resolve as an OpenAPI impl.
"""
directive @openapi_namespace on FIELD_DEFINITION

"""
Used by core to mark a type as being one of the generated TAgg types. Not for general use!
"""
directive @isagg(type: String!) on OBJECT | ENUM

"""
Adding this to any keyword indexed field ensures that no duplicates can be inserted for that field
"""
directive @unique on FIELD_DEFINITION

"""Trigger the execution of the workflow with the given name"""
directive @workflow(name: String, inline: WorkflowInput) on FIELD_DEFINITION

"""
check whether the string is conform to the email address specification
Matches RFC 5322 regex, probably most relevant https://tools.ietf.org/html/rfc5322#section-3.4.1
pattern available at https://stackoverflow.com/a/201378/400048
"""
directive @email on FIELD_DEFINITION

"""If present on an object no input type will be generated from that type"""
directive @noinput on OBJECT

"""
Added to any field which should be store encrypted and not allowed to be returned to clients via GraphQL
PKCS5 is decrypted and can be passed to other functions but will not be returned through the generated APIs.
Note that only PKCS5 is reversible. Other encrypted formats are ONE WAY meaning you can compare new values against the
encrypted hash but you cannot get or compare to the original directly.
"""
directive @secret(hash: HashAlgorithm) on FIELD_DEFINITION

"""
A trigger allows dynamically triggering functions before and after events on any Query or Mutation function
Triggers must be defined in schema because dynamic definitions at runtime would have a negative performance impact
Since it would require the platform to lookup and keep track of triggers per instance.
A trigger can be applied to inherited functions by redefining the function with exactly the same arguments, return type and directives + adding the trigger
If applied to an object the before and after triggers are executed before and after any upsert of that type
"""
directive @trigger(config: Trigger!) on OBJECT | FIELD_DEFINITION

"""
When added to an Int or Float field, the math arguments will not be added to that field
"""
directive @nomath on OBJECT | FIELD_DEFINITION

"""
check if the field's value matches the regular expression given a match flag (see https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html  )
if allMustMatch is true (default) then all regex must pass
"""
directive @pattern(regex: [String!]!, allMustMatch: Boolean = true) on FIELD_DEFINITION

type AggregatedPolicyAggs {
  decisionStrategy: AggOtherScalar
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type OAuth2AuthorizedClientAggs {
  clientRegistrationId: AggOtherScalar
  principalName: AggOtherScalar
  accessToken: AggOtherScalar
  refreshToken: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input WorkflowExecutableAsInputOpt {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInputOpt
  repeatN: Int
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
}

"""All fields defined by MessageGroup"""
enum MessageGroupFields {
  hypi
  members
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type Query {
  """
  Find objects of the given type which match the query. Use a conditional fragment to select fields as in https://graphql.org/learn/schema/#union-types
  """
  find(
    type: HypiMutationType!
    arcql: String!
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): HypiFilterConnection!

  """Gets a single object by its ID (hypi.id)"""
  get(type: HypiMutationType!, id: String!): HypiRootAggregate
  aggregate: HypiAggregationType
  login(username: String!, password: String!): AccessToken
  loginByEmail(email: String!, password: String!): AccessToken
  me: PermissionDescription
  hasPermission(req: [PermissionRequest!]!): [Boolean]
}

input AddressInput {
  hypi: HypiInput
  door: String
  street: String
  town: String
  county: String
  city: String
  country: CountryInput
  postCode: String
  from: DateTime
  to: DateTime
}

"""All fields defined by PersonName"""
enum PersonNameFields {
  hypi
  title
  firstName
  lastName
  from
  to
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""
input ComputedAggField {
  field: String! #The name of the field in the GraphQL type
  projectTo: String #The name of the field that should receive the aggregated value - if null, the value is projected to the same field IFF the type is Int or Float
  agg: ComputedAggType #Defaults to COUNT
}
"""
enum ComputedAggType {
  COUNT
  SUM
  AVG
  MAX
  MIN
}

enum Gender {
  MALE
  FEMALE
  OTHER
  RATHER_NOT_SAY
}

type NotificationCtx {
  hypi: Hypi

  """The type that the notification applies to"""
  type: String
  targetAccount: ID
}

enum AuthenticationMethod {
  header
  form
  query
}

enum UserNameAttributeName {
  iss
  sub
  aud
  exp
  iat
  auth_time
  nonce
  acr
  amr
  azp
  at_hash
  c_hash
}

input GroupInputOpt {
  hypi: HypiInputOpt
  name: String
  accounts: [AccountInputOpt]
  children: [GroupInputOpt]
  organisations: [OrganisationInputOpt]
}

""" Logging"""
enum LogLevel {
  DEBUG
  INFO
  WARN
  ERROR
}

"""All fields defined by File"""
enum FileFields {
  hypi
  name
  directory
  path
  isDirectory
  status
  url

  """mime type"""
  type
  size
  extension
  isStared
  isSharable
  content
  children
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type PhoneAggs {
  number: AggOtherScalar
  code: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Defines the templates that should be applied to a given HTTP request"""
type RequestTemplate {
  hypi: Hypi
  name: String!
  request: String
  response: String
}

input GroupPolicyInputOpt {
  hypi: HypiInputOpt
  groups: [GroupInputOpt]
  name: String
  logic: AuthLogic
}

"""
To reset an Account's password, create a `PasswordReminder`.

This will generate a code in the `code` field that can be referenced using $!{parent.code} in the `htmlMessage` or `plainTextMessage` fields.

This will send an email to the email in the `to` field. In the message you should provide a link to a URL where the user can enter their new password.
Include the code in this URL e.g. https://my-app.com/reset-password?code=$!{parent.code}.

When the user gets to this page, you will have the password reset code in the URL query string. Get this code from the URL
and when the user enter their new password, make a POST request to the Hypi API e.g.
POST <hypi-domain>/email/reset/<domain> - where <domain> is app instance domain.

In the body of the request send a JSON like this:
{"code": "<the-code-from-the-URL>", "password": "<the-user's-new-password>"}

Hypi will change the user's password and return HTTP status 200.
"""
type PasswordReminder {
  hypi: Hypi

  """If true the reset code has not yet been used."""
  valid: Boolean

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code: String

  """The Account email that needs to be changed"""
  to: Email!

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from: String

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject: String

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage: String

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage: String
}

input AccountInput {
  hypi: HypiInput
  emails: [EmailInput!]
  verified: Boolean
  enabled: Boolean
  phones: [PhoneInput!]
  username: String!
  password: PasswordInput!
  owner: PersonInput
  groups: [GroupInput!]
  roles: [RoleInput!]
  attempts: [LoginAttemptInput!]
  remoteLogins: [RemoteLoginInput!]
}

input EmailSendingAttemptGroupByOptions {
  """The field by which to to group the matching data"""
  field: EmailSendingAttemptScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by AccountPolicy"""
enum AccountPolicyScalarFields {
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by Group"""
enum GroupFields {
  hypi

  """
  A unique name identifying this group, implicitly sets the path of the group to /<name> whihc can be referenced in wild card permissions
  """
  name
  accounts
  children
  organisations
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by Country"""
enum CountryFields {
  hypi
  name
  stateName
  sovereignty
  alpha2code
  alpha3code
  numericCode
  subdivisionCodeLinks
  internetCCTLD
  continent
  officialLanguage
  currencies
  languagesSpoken
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by RealmLink"""
enum RealmLinkFields {
  hypi
  name
  accounts
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""
If the query or mutation functions in the Webhook definition returns this then it controls what the server responds with
For example, the GraphQL function can return a 301 or 302 status and a Location header to an external URL to cause a redirect.
"""
type WebhookResponse {
  hypi: Hypi
  status: Int
  headers: Json
  body: Json
}

type EmailSendingAttemptAggs {
  headers: AggOtherScalar
  body: AggOtherScalar
  status: AggOtherScalar
  statusMessage: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""
A union of all types in the app which can be created or updated directly
"""
union HypiRootAggregate = Message | User | MessageGroup | UserStory | PageInfo | HypiResultEdge | HypiFilterConnection | HypiEnv | Pair | AggInt | AggFloat | AggOtherScalar | Script | RequestTemplate | NotificationCtx | Notification | URL | Currency | Coordinate | GeoEnvelope | Language | Address | PersonName | Phone | Email | Password | RemoteLogin | LoginAttempt | BruteForceDetectionOptions | OAuth2AuthorizedClient | AuthClient | ABACPolicy | ABACTag | Image | EmailVerification | EmailTemplate | EmailSendingAttempt | PasswordReminder | Webhook | WebhookResponse | LogMessage | GraphQLRef | WorkflowStepData | WorkflowStep | AccessToken | StorageCounter | PermissionDescription | Hypi | Country | Account | Person | Organisation | OAuthProvider | Realm | Group | Role | RolePolicy | ClientPolicy | TimePolicy | AggregatedPolicy | GroupPolicy | AccountPolicy | RealmPolicy | RealmLink | Permission | File | Video | EmailMessage | Workflow | WorkflowSession | Counter | Gauge | ServerlessResponse

"""An object available as "env" in all scripts"""
type HypiEnv {
  hypi: Hypi
  apiHost: String
  websocketHost: String
}

input EmailMessageGroupByOptions {
  """The field by which to to group the matching data"""
  field: EmailMessageScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input WorkflowOrderedInput {
  execAs: String
  async: Boolean
  name: String
  fn: GraphQLRefInput
  maxExecutionTime: String
  repeatN: Int
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int!
  repeatIf: GraphQLRefInput
}

input WorkflowGroupByOptions {
  """The field by which to to group the matching data"""
  field: WorkflowScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by Video"""
enum VideoFields {
  hypi
  name
  file
  description
  location
  thumbnails
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input OAuth2AuthorizedClientGroupByOptions {
  """The field by which to to group the matching data"""
  field: OAuth2AuthorizedClientScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type AccountAggs {
  verified: AggOtherScalar
  enabled: AggOtherScalar
  username: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

enum HttpMethod {
  GET
  PUT
  POST
  DELETE
  PATCH
  OPTIONS
  HEAD
  TRACE
}

input ServerlessResponseInputOpt {
  hypi: HypiInputOpt
  path: String
  headers: Json
  multiPart: Boolean
  method: String
  files: [FileInputOpt]
  chunked: Boolean
  attributes: [String]
  queryString: Json
  body: Json
  cookies: Json
}

input BruteForceDetectionOptionsInput {
  hypi: HypiInput
  maxLoginFailures: Int!
  waitIncrements: Int
  waitIncrementsUnit: TimeUnit
  quickLoginCheckMillis: Int
  minQuickLoginWait: Int
  minQuickLoginWaitUnit: TimeUnit
  maxWait: Int
  maxWaitUnit: TimeUnit
  failureReset: Int
  failureResetUnit: TimeUnit
}

input NotificationCtxInput {
  hypi: HypiInput
  type: String
  targetAccount: ID
}

input FileInput {
  hypi: HypiInput
  name: String!
  directory: String!
  path: String!
  isDirectory: Boolean!
  status: FileStatus
  url: URLInput
  children: [FileInput!]
  type: String
  size: Long
  extension: String
  isStared: Boolean
  isSharable: Boolean
  content: String
}

input GraphQLRefInput {
  hypi: HypiInput
  type: OpType!
  field: String!
  selection: String
}

"""
Events that can be raised by the system to inform you about errors or other state that happened asynchronously, hence you may not otherwise have been told this info.
"""
type Notification {
  hypi: Hypi
  message: String
  ctx: NotificationCtx
}

input CurrencyInputOpt {
  hypi: HypiInputOpt
  name: String
  code: String
  symbol: String
}

type VideoAggs {
  name: AggOtherScalar
  description: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input NotificationCtxGroupByOptions {
  """The field by which to to group the matching data"""
  field: NotificationCtxScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input RemoteLoginInput {
  hypi: HypiInput
  type: String
  email: String
  remoteId: String
  otherAttributes: Json
}

input RolePolicyInput {
  hypi: HypiInput
  roles: [RoleInput!]!
  name: String!
  logic: AuthLogic
}

type AggregatedPolicy implements Policy {
  hypi: Hypi

  """defines how the policy arrives at a decision, the options are:"""
  decisionStrategy: DecisionStrategy
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  policies(arcql: String, first: Int, after: String, last: Int, before: String): [Policy!]!
}

input ABACPolicyInput {
  hypi: HypiInput
  from: DateTime
  to: DateTime
  givenInstance: String
  givenType: String!
  givenOperation: String!
  givenFn: String
  givenFnPrefix: String
  whenResourceTagKeyEq: String
  whenResourceTagKeyPrefix: String
  whenResourceTagValueEq: String
  whenResourceTagValuePrefix: String
  assertAccountIdEq: String
  assertAccountUsernamePrefix: String
  assertAccountTagKeyEq: String
  assertAccountTagKeyPrefix: String
  assertAccountTagValEq: String
  assertAccountTagValPrefix: String
  boundary: PolicyBoundary
  allowedFields: String
}

"""All fields defined by Phone"""
enum PhoneFields {
  hypi
  number
  country
  code
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input AccountGroupByOptions {
  """The field by which to to group the matching data"""
  field: AccountScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""
Indicates the type should be repeated once it's finished IFF the referenced function is defined AND executing it returns true;
repeatIf and repeatN are mutually exclusive. If repeatN is specified it takes precedence over repeatIf
"""
interface WorkflowRepeatable {
  hypi: Hypi
  repeatIf: GraphQLRef
  repeatN: Int
}

"""All fields defined by User"""
enum UserFields {
  hypi
  account
  stories
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type ServerlessResponse {
  hypi: Hypi
  path: String
  headers: Json
  multiPart: Boolean
  method: String
  chunked: Boolean
  queryString: Json
  body: Json
  cookies: Json
  files(arcql: String, first: Int, after: String, last: Int, before: String): [File]
  attributes(arcql: String, first: Int, after: String, last: Int, before: String): [String]
}

input StorageCounterInputOpt {
  hypi: HypiInputOpt
  type: String
  field: String
  size: Int
}

"""@deprecated - to be removed"""
type RealmLink {
  hypi: Hypi
  name: String! @deprecated(reason: "RealmLink will be removed in a future version")
  accounts(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]! @deprecated(reason: "RealmLink will be removed in a future version")
}

"""All fields defined by RolePolicy"""
enum RolePolicyFields {
  hypi
  name

  """Positive` or `Negative"""
  logic
  roles
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by Workflow"""
enum WorkflowFields {
  hypi
  name

  """
  If present, this is a cron schedule to automatically execute this Workflow
  The syntax as defined at https://www.manpagez.com/man/5/crontab/
  NOTE: The special strings @hourly, @daily etc are NOT supported
  """
  cronSchedule

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs
  async

  """
  If present AND true, all steps in this block are executed at the same time.
  """
  parallel

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime
  repeatN
  evaluateIf
  repeatIf
  steps
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
  evaluateIf_hypi
  evaluateIf_type
  evaluateIf_field
  evaluateIf_selection
  repeatIf_hypi
  repeatIf_type
  repeatIf_field
  repeatIf_selection
}

"""All fields defined by ABACPolicy"""
enum ABACPolicyFields {
  hypi
  from
  to

  """
   The instance the policy applies to. By default the same instance in which it exists.
   A `Platform Admin` can set it to *, all other users get permission denied if this is not the same as their current instance.
   
  """
  givenInstance

  """e.g. Account or *"""
  givenType

  """Exactly one of Query|Mutation|Subscription|*"""
  givenOperation

  """The exact function name or wildcard e.g. find|upsert|*"""
  givenFn

  """
  The prefix that any function can begin with e.g. find will match findX, findY, findOther
  """
  givenFnPrefix
  whenResourceTagKeyEq
  whenResourceTagKeyPrefix
  whenResourceTagValueEq
  whenResourceTagValuePrefix

  """Policy applies when the account ID is equal to this"""
  assertAccountIdEq

  """Policy applies when the account username starts with this"""
  assertAccountUsernamePrefix

  """When set, the account MUST have a tag whose key is equal to this"""
  assertAccountTagKeyEq

  """When set, the account MUST have a tag whose key is starts with this"""
  assertAccountTagKeyPrefix

  """When set, the account MUST have a tag whose value is equal to this"""
  assertAccountTagValEq

  """When set, the account MUST have a tag whose value is starts with this"""
  assertAccountTagValPrefix

  """
  Resource owner can set the boundary to RESOURCE (or anyone that has permission to do so)
  System Admin for the instance can set the boundary to INSTANCE
  Platform Admin can set the boundary to PLATFORM
  PLATFORM|INSTANCE|RESOURCE
  """
  boundary

  """
  If provided, this is a comma separate list of field paths that are allowed by this policy
  e.g. a,b.c allows access to a and all sub-fields below it as well as to the field c under the parent field b. No other field under b is allowed
  If the policy is allowing read access, only these fields can be seen. If it is write acces, only these fields can be modified.
  """
  allowedFields
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input WorkflowStepDataInput {
  hypi: HypiInput
  stepName: String!
  stepResult: Any!
}

input ABACPolicyInputOpt {
  hypi: HypiInputOpt
  from: DateTime
  to: DateTime
  givenInstance: String
  givenType: String
  givenOperation: String
  givenFn: String
  givenFnPrefix: String
  whenResourceTagKeyEq: String
  whenResourceTagKeyPrefix: String
  whenResourceTagValueEq: String
  whenResourceTagValuePrefix: String
  assertAccountIdEq: String
  assertAccountUsernamePrefix: String
  assertAccountTagKeyEq: String
  assertAccountTagKeyPrefix: String
  assertAccountTagValEq: String
  assertAccountTagValPrefix: String
  boundary: PolicyBoundary
  allowedFields: String
}

"""Scalar fields defined by Notification"""
enum NotificationScalarFields {
  message
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  ctx_type
  ctx_targetAccount
}

input RealmLinkInput {
  hypi: HypiInput
  name: String!
  accounts: [AccountInput!]!
}

type LoginAttempt {
  hypi: Hypi
  successful: Boolean
  errorCode: String
}

input CountryInputOpt {
  hypi: HypiInputOpt
  name: String
  stateName: String
  sovereignty: String
  alpha2code: String
  alpha3code: String
  numericCode: String
  subdivisionCodeLinks: String
  internetCCTLD: String
  continent: String
  currencies: [CurrencyInputOpt]
  languagesSpoken: [LanguageInputOpt]
  officialLanguage: LanguageInputOpt
}

type ImageAggs {
  name: AggOtherScalar
  description: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input AccessTokenInputOpt {
  hypi: HypiInputOpt
  sessionToken: String
  sessionExpires: Long
  errorCode: String
  errorMsg: String
}

input PolicyInputOpt {
  realms: [RealmLinkInputOpt]
  clients: [AuthClientInputOpt]
  roles: [RoleInputOpt]
  name: String
  policies: [PolicyInputOpt]
  groups: [GroupInputOpt]
  from: DateTime
  hypi: HypiInputOpt
  logic: AuthLogic
  to: DateTime
  accounts: [AccountInputOpt]
  decisionStrategy: DecisionStrategy
}

"""@deprecated - to be removed"""
type RealmPolicy implements Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  realms(arcql: String, first: Int, after: String, last: Int, before: String): [RealmLink!] @deprecated(reason: "RealmPolicy will be removed in a future version")
}

input PermissionRequest {
  opType: OpType!
  operationName: String!
  type: String!
  scopes: [String!]!
  resource: String!
  from: DateTime
  to: DateTime
}

type Email {
  hypi: Hypi
  value: String!
  type: String
}

interface WorkflowOrdered {
  hypi: Hypi
  order: Int!
}

input CoordinateInput {
  hypi: HypiInput
  x: Float!
  y: Float!
}

"""All fields defined by HypiEnv"""
enum HypiEnvFields {
  hypi
  apiHost
  websocketHost
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input WorkflowOrderedInputOpt {
  execAs: String
  async: Boolean
  name: String
  fn: GraphQLRefInputOpt
  maxExecutionTime: String
  repeatN: Int
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
  repeatIf: GraphQLRefInputOpt
}

"""Scalar fields defined by User"""
enum UserScalarFields {
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type OAuthProvider {
  hypi: Hypi

  """ instanceId-(hypi.id = registrationId)"""
  clientId: String!
  clientSecret: String!
  clientAuthenticationMethod: ClientAuthenticationMethod
  authorizationGrantType: AuthorizationGrantType
  redirectUriTemplate: String
  authorizationUri: String
  tokenUri: String
  userInfoUri: String
  userInfoAuthenticationMethod: AuthenticationMethod
  userNameAttributeName: UserNameAttributeName
  jwkSetUri: String
  clientName: String
  hypiSuccessRedirectUri: String
  hypiFailureRedirectUri: String
  scopes(arcql: String, first: Int, after: String, last: Int, before: String): [String!]
  configurationMetadata(arcql: String, first: Int, after: String, last: Int, before: String): [Pair!]
}

"""Scalar fields defined by AggregatedPolicy"""
enum AggregatedPolicyScalarFields {
  """defines how the policy arrives at a decision, the options are:"""
  decisionStrategy
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type HypiResultEdge {
  hypi: Hypi
  node: HypiRootAggregate!
  cursor: ID!
}

input GaugeMaths {
  value: MathInputFloat
}

"""All fields defined by Realm"""
enum RealmFields {
  hypi

  """
  the name identifying the organisation and becomes the URL by which it is accessed e.g. alpha-corp.hypi.app, where alpha-corp is name
  If not provided one will be automatically generated
  """
  name
  logo

  """The name displayed in the user interface"""
  displayName

  """If true users can register without an admin creating their account"""
  allowRegistrations

  """if true users must verify their email before they're allowed to login"""
  verifyEmail

  """
  Optionally defines some options to help detect and protect against brute force login attempts
  """
  bruteForceDetection
  referrer
  remoteLoginId
  organisations
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type ABACPolicyAggs {
  from: AggOtherScalar
  to: AggOtherScalar
  givenInstance: AggOtherScalar
  givenType: AggOtherScalar
  givenOperation: AggOtherScalar
  givenFn: AggOtherScalar
  givenFnPrefix: AggOtherScalar
  whenResourceTagKeyEq: AggOtherScalar
  whenResourceTagKeyPrefix: AggOtherScalar
  whenResourceTagValueEq: AggOtherScalar
  whenResourceTagValuePrefix: AggOtherScalar
  assertAccountIdEq: AggOtherScalar
  assertAccountUsernamePrefix: AggOtherScalar
  assertAccountTagKeyEq: AggOtherScalar
  assertAccountTagKeyPrefix: AggOtherScalar
  assertAccountTagValEq: AggOtherScalar
  assertAccountTagValPrefix: AggOtherScalar
  boundary: AggOtherScalar
  allowedFields: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input EmailTemplateInputOpt {
  hypi: HypiInputOpt
  name: String
  description: String
  template: String
  comment: String
}

input BruteForceDetectionOptionsGroupByOptions {
  """The field by which to to group the matching data"""
  field: BruteForceDetectionOptionsScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input AccountInputOpt {
  hypi: HypiInputOpt
  emails: [EmailInputOpt]
  verified: Boolean
  enabled: Boolean
  phones: [PhoneInputOpt]
  username: String
  password: PasswordInputOpt
  owner: PersonInputOpt
  groups: [GroupInputOpt]
  roles: [RoleInputOpt]
  attempts: [LoginAttemptInputOpt]
  remoteLogins: [RemoteLoginInputOpt]
}

input AccountPolicyInputOpt {
  hypi: HypiInputOpt
  accounts: [AccountInputOpt]
  name: String
  logic: AuthLogic
}

"""All fields defined by WorkflowSession"""
enum WorkflowSessionFields {
  hypi
  data
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input CounterMaths {
  value: MathInputFloat
}

type ABACPolicy {
  hypi: Hypi
  from: DateTime
  to: DateTime

  """
   The instance the policy applies to. By default the same instance in which it exists.
   A `Platform Admin` can set it to *, all other users get permission denied if this is not the same as their current instance.
   
  """
  givenInstance: String

  """e.g. Account or *"""
  givenType: String!

  """Exactly one of Query|Mutation|Subscription|*"""
  givenOperation: String!

  """The exact function name or wildcard e.g. find|upsert|*"""
  givenFn: String

  """
  The prefix that any function can begin with e.g. find will match findX, findY, findOther
  """
  givenFnPrefix: String
  whenResourceTagKeyEq: String
  whenResourceTagKeyPrefix: String
  whenResourceTagValueEq: String
  whenResourceTagValuePrefix: String

  """Policy applies when the account ID is equal to this"""
  assertAccountIdEq: String

  """Policy applies when the account username starts with this"""
  assertAccountUsernamePrefix: String

  """When set, the account MUST have a tag whose key is equal to this"""
  assertAccountTagKeyEq: String

  """When set, the account MUST have a tag whose key is starts with this"""
  assertAccountTagKeyPrefix: String

  """When set, the account MUST have a tag whose value is equal to this"""
  assertAccountTagValEq: String

  """When set, the account MUST have a tag whose value is starts with this"""
  assertAccountTagValPrefix: String

  """
  Resource owner can set the boundary to RESOURCE (or anyone that has permission to do so)
  System Admin for the instance can set the boundary to INSTANCE
  Platform Admin can set the boundary to PLATFORM
  PLATFORM|INSTANCE|RESOURCE
  """
  boundary: PolicyBoundary

  """
  If provided, this is a comma separate list of field paths that are allowed by this policy
  e.g. a,b.c allows access to a and all sub-fields below it as well as to the field c under the parent field b. No other field under b is allowed
  If the policy is allowing read access, only these fields can be seen. If it is write acces, only these fields can be modified.
  """
  allowedFields: String
}

input GraphQLRefInputOpt {
  hypi: HypiInputOpt
  type: OpType
  field: String
  selection: String
}

input BruteForceDetectionOptionsInputOpt {
  hypi: HypiInputOpt
  maxLoginFailures: Int
  waitIncrements: Int
  waitIncrementsUnit: TimeUnit
  quickLoginCheckMillis: Int
  minQuickLoginWait: Int
  minQuickLoginWaitUnit: TimeUnit
  maxWait: Int
  maxWaitUnit: TimeUnit
  failureReset: Int
  failureResetUnit: TimeUnit
}

type PermissionAggs {
  name: AggOtherScalar
  decisionStrategy: AggOtherScalar
  type: AggOtherScalar
  resource: AggOtherScalar
  operationType: AggOtherScalar
  includeAllAccounts: AggOtherScalar
  scopes: AggOtherScalar
  operations: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type ServerlessResponseAggs {
  path: AggOtherScalar
  headers: AggOtherScalar
  multiPart: AggOtherScalar
  method: AggOtherScalar
  chunked: AggOtherScalar
  queryString: AggOtherScalar
  body: AggOtherScalar
  cookies: AggOtherScalar
  attributes: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input RealmInput {
  hypi: HypiInput
  name: String
  logo: ImageInput
  displayName: String
  allowRegistrations: Boolean
  verifyEmail: Boolean
  bruteForceDetection: BruteForceDetectionOptionsInput
  organisations: [OrganisationInput!]!
  referrer: String
  remoteLoginId: String
}

input TimePolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: TimePolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type Role {
  hypi: Hypi
  name: String!
  accounts(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]
}

type RolePolicy implements Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  roles(arcql: String, first: Int, after: String, last: Int, before: String): [Role!]!
}

input InlineHttpRequestTemplate {
  requestTemplate: String
  responseTemplate: String
}

type FileAggs {
  name: AggOtherScalar
  directory: AggOtherScalar
  path: AggOtherScalar
  isDirectory: AggOtherScalar
  status: AggOtherScalar
  type: AggOtherScalar
  size: AggOtherScalar
  extension: AggOtherScalar
  isStared: AggOtherScalar
  isSharable: AggOtherScalar
  content: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input PhoneGroupByOptions {
  """The field by which to to group the matching data"""
  field: PhoneScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All numeric fields defined by Workflow"""
enum WorkflowNumericFields {
  repeatN
}

type Video {
  hypi: Hypi
  name: String!
  file: File!
  description: String
  location: Geo
  thumbnails(arcql: String, first: Int, after: String, last: Int, before: String): [Image!]
}

input ABACTagInputOpt {
  hypi: HypiInputOpt
  key: String
  value: String
}

"""All fields defined by Password"""
enum PasswordFields {
  hypi

  """
   password is never returned
   further, the @secret directive enforces this, queries can be use to perform comparison against the field but it is never returned
   
  """
  value
  expired
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by ServerlessResponse"""
enum ServerlessResponseScalarFields {
  path
  headers
  multiPart
  method
  chunked
  queryString
  body
  cookies
  attributes
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input VideoGroupByOptions {
  """The field by which to to group the matching data"""
  field: VideoScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type NotificationCtxAggs {
  type: AggOtherScalar
  targetAccount: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input WorkflowStepDataGroupByOptions {
  """The field by which to to group the matching data"""
  field: WorkflowStepDataScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All numeric fields defined by Coordinate"""
enum CoordinateNumericFields {
  x
  y
}

input WebhookPayload {
  url: URLInput!
  headers: Json!
  body: String
}

input WorkflowParallelInputOpt {
  execAs: String
  async: Boolean
  parallel: Boolean
  name: String
  maxExecutionTime: String
  repeatN: Int
  hypi: HypiInputOpt
  steps: [WorkflowStepInputOpt]
  evaluateIf: GraphQLRefInputOpt
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
}

enum ClientAuthenticationMethod {
  basic
  post
}

type MessageGroupAggs {
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by RealmLink"""
enum RealmLinkScalarFields {
  name
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All numeric fields defined by Gauge"""
enum GaugeNumericFields {
  value
}

type UserStory {
  hypi: Hypi
  text: String
  img: String
}

"""Measures a value at a point in time"""
type Gauge {
  hypi: Hypi

  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name: String!

  """A human friendly display label for the counter"""
  label: String

  """
  The current value of this gauge, set, increase or decrease as you see fit
  """
  value: Float!
  tags(arcql: String, first: Int, after: String, last: Int, before: String): [String!]
}

type AddressAggs {
  door: AggOtherScalar
  street: AggOtherScalar
  town: AggOtherScalar
  county: AggOtherScalar
  city: AggOtherScalar
  postCode: AggOtherScalar
  from: AggOtherScalar
  to: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by Workflow"""
enum WorkflowScalarFields {
  name

  """
  If present, this is a cron schedule to automatically execute this Workflow
  The syntax as defined at https://www.manpagez.com/man/5/crontab/
  NOTE: The special strings @hourly, @daily etc are NOT supported
  """
  cronSchedule

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs
  async

  """
  If present AND true, all steps in this block are executed at the same time.
  """
  parallel

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime
  repeatN
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  evaluateIf_type
  evaluateIf_field
  evaluateIf_selection
  repeatIf_type
  repeatIf_field
  repeatIf_selection
}

"""
==============IaM models==============
union Entity = Account | Organisation #todo unions not yet supported
"""
type Language {
  hypi: Hypi
  family: String
  isoName: String
  nativeName: String
  iso6391Code: String
  iso6392TCode: String
  iso6392BCode: String
  iso6393Code: String
}

input ImageInput {
  hypi: HypiInput
  name: String!
  file: FileInput!
  description: String
  location: GeoInput
}

"""All fields defined by Currency"""
enum CurrencyFields {
  hypi
  name
  code
  symbol
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input OAuthProviderGroupByOptions {
  """The field by which to to group the matching data"""
  field: OAuthProviderScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input NotificationCtxInputOpt {
  hypi: HypiInputOpt
  type: String
  targetAccount: ID
}

"""Scalar fields defined by ClientPolicy"""
enum ClientPolicyScalarFields {
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type PersonName {
  hypi: Hypi
  title: String
  firstName: String
  lastName: String
  from: DateTime
  to: DateTime
}

type EmailAggs {
  value: AggOtherScalar
  type: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input GraphQLRefGroupByOptions {
  """The field by which to to group the matching data"""
  field: GraphQLRefScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input LanguageInputOpt {
  hypi: HypiInputOpt
  family: String
  isoName: String
  nativeName: String
  iso6391Code: String
  iso6392TCode: String
  iso6392BCode: String
  iso6393Code: String
}

"""
Used to represent arbitrary customer specific collections of entities e.g. an Organisation, a Meetup or Friends
"""
type Organisation {
  hypi: Hypi
  name: String!
  logo: Image
  incorporated: DateTime
  addresses(arcql: String, first: Int, after: String, last: Int, before: String): [Address!]
  phones(arcql: String, first: Int, after: String, last: Int, before: String): [Phone!]
  emails(arcql: String, first: Int, after: String, last: Int, before: String): [Email!]
  members(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]
  subsidiaries(arcql: String, first: Int, after: String, last: Int, before: String): [Organisation!]
}

input OrganisationInputOpt {
  hypi: HypiInputOpt
  name: String
  logo: ImageInputOpt
  addresses: [AddressInputOpt]
  incorporated: DateTime
  phones: [PhoneInputOpt]
  emails: [EmailInputOpt]
  members: [AccountInputOpt]
  subsidiaries: [OrganisationInputOpt]
}

"""All fields defined by ClientPolicy"""
enum ClientPolicyFields {
  hypi
  name

  """Positive` or `Negative"""
  logic
  clients
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type UserStoryAggs {
  text: AggOtherScalar
  img: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input LoginAttemptInput {
  hypi: HypiInput
  successful: Boolean
  errorCode: String
}

type WorkflowSession {
  hypi: Hypi
  data(arcql: String, first: Int, after: String, last: Int, before: String): [WorkflowStepData!]
}

input UserInput {
  hypi: HypiInput
  account: AccountInput!
  stories: [UserStoryInput!]
}

input PermissionGroupByOptions {
  """The field by which to to group the matching data"""
  field: PermissionScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input HttpResponseInput {
  headers: Json
  rawPayload: String
  hypi: HypiInput
  status: Int
}

input GroupPolicyInput {
  hypi: HypiInput
  groups: [GroupInput!]
  name: String!
  logic: AuthLogic
}

input WorkflowRepeatableInputOpt {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInputOpt
  repeatN: Int
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
}

"""See https://relay.dev/graphql/connections.htm#sec-undefined.PageInfo"""
type PageInfo {
  hypi: Hypi
  hasPreviousPage: Boolean!
  hasNextPage: Boolean!
  startCursor: ID!

  """relay modern - https://relay.dev/graphql/connections.htm#note-95f8a"""
  endCursor: ID!

  """
  relay modern
  If present, is the page size that is used to generate the previousOffsets and nextOffsets if they're present
  """
  pageLimit: Int

  """
  If present, contains the pagination cursors that can be used to get the previous N pages
  """
  previousOffsets: [String!]

  """
  If present, contains the pagination cursors that can be used to get the next N pages
  """
  nextOffsets: [String!]
}

input GeoInput {
  envelope: GeoEnvelopeInput
  hypi: HypiInput
  srid: Int
}

input AggregatedPolicyInput {
  hypi: HypiInput
  policies: [PolicyInput!]!
  decisionStrategy: DecisionStrategy
  name: String!
  logic: AuthLogic
}

type AuthClientAggs {
  name: AggOtherScalar
  secret: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

enum PolicyBoundary {
  PLATFORM
  INSTANCE
  RESOURCE
}

input EmailSendingAttemptInputOpt {
  hypi: HypiInputOpt
  headers: Json
  body: Json
  status: EmailEventType
  statusMessage: String
}

input LogMessageInputOpt {
  hypi: HypiInputOpt
  level: LogLevel
  message: String
  stackTrace: String
  releaseId: String
  type: String
  workflow: String
}

input GeoEnvelopeGroupByOptions {
  """The field by which to to group the matching data"""
  field: GeoEnvelopeScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input EmailGroupByOptions {
  """The field by which to to group the matching data"""
  field: EmailScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by Webhook"""
enum WebhookFields {
  hypi

  """
  The name by which this web hook is referenced in the URL, if missing the webhook is only adressable by ID
  """
  name

  """
   Defaults to the account creating the Webhook.
   Hypi will generate an authorisation token automatically for the account when the web hook is triggered.
   This token will then be used to execute the triggers in the web hook (query or mutation).
   For security an account should be created specifically for invoking web hooks and an AccountPolicy should be created that grants access only to the specified functions or otherwise limit the scope of what the account can do.
   
  """
  as

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_hypi

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_type

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_field

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_selection
}

type Address {
  hypi: Hypi
  door: String
  street: String
  town: String
  county: String
  city: String
  country: Country
  postCode: String

  """  country: Country"""
  from: DateTime
  to: DateTime
}

"""Scalar fields defined by WebhookResponse"""
enum WebhookResponseScalarFields {
  status
  headers
  body
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input WorkflowStepInput {
  hypi: HypiInput
  name: String
  fn: GraphQLRefInput!
  order: Int!
  execAs: String
  async: Boolean
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRefInput
  repeatIf: GraphQLRefInput
}

input VideoInput {
  hypi: HypiInput
  name: String!
  thumbnails: [ImageInput!]
  file: FileInput!
  description: String
  location: GeoInput
}

"""Scalar fields defined by Script"""
enum ScriptScalarFields {
  type
  name
  body
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input LogMessageGroupByOptions {
  """The field by which to to group the matching data"""
  field: LogMessageScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input RealmGroupByOptions {
  """The field by which to to group the matching data"""
  field: RealmScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input OAuthProviderInput {
  hypi: HypiInput
  clientId: String!
  clientSecret: String!
  clientAuthenticationMethod: ClientAuthenticationMethod
  authorizationGrantType: AuthorizationGrantType
  redirectUriTemplate: String
  scopes: [String!]
  authorizationUri: String
  tokenUri: String
  userInfoUri: String
  userInfoAuthenticationMethod: AuthenticationMethod
  userNameAttributeName: UserNameAttributeName
  jwkSetUri: String
  configurationMetadata: [PairInput!]
  clientName: String
  hypiSuccessRedirectUri: String
  hypiFailureRedirectUri: String
}

type MessageGroup {
  hypi: Hypi
  members(arcql: String, first: Int, after: String, last: Int, before: String): [User!]
}

input UserStoryGroupByOptions {
  """The field by which to to group the matching data"""
  field: UserStoryScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type Image {
  hypi: Hypi
  name: String!
  file: File!
  description: String
  location: Geo
}

"""DateTime scalar"""
scalar DateTime

type ClientPolicyAggs {
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input PairInputOpt {
  hypi: HypiInputOpt
  key: String
  value: String
}

input ClientPolicyInputOpt {
  hypi: HypiInputOpt
  clients: [AuthClientInputOpt]
  name: String
  logic: AuthLogic
}

input CountryInput {
  hypi: HypiInput
  name: String!
  stateName: String
  sovereignty: String
  alpha2code: String
  alpha3code: String
  numericCode: String
  subdivisionCodeLinks: String
  internetCCTLD: String
  continent: String
  currencies: [CurrencyInput!]
  languagesSpoken: [LanguageInput!]
  officialLanguage: LanguageInput
}

type WebhookAggs {
  name: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  query_type: AggOtherScalar
  query_field: AggOtherScalar
  query_selection: AggOtherScalar
}

input CoordinateInputOpt {
  hypi: HypiInputOpt
  x: Float
  y: Float
}

type OrganisationAggs {
  name: AggOtherScalar
  incorporated: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""
One of Hypi's supported Scripting languages. Currently:
1. Groovy
2. JavaScript

Or full serverless functions via KNative, support for OpenWhisk is to come in a future release
"""
enum TanType {
  Groovy

  """https://velocity.apache.org/engine/2.2/user-guide.html"""
  Velocity

  """
  Any OpenFaaS function can be used. See a python example at https://docs.openfaas.com/tutorials/first-python-function/
  """
  OpenWhisk
  OpenFaaS
}

input ScriptInputOpt {
  hypi: HypiInputOpt
  type: TanType
  name: String
  body: String
}

"""Scalar fields defined by Country"""
enum CountryScalarFields {
  name
  stateName
  sovereignty
  alpha2code
  alpha3code
  numericCode
  subdivisionCodeLinks
  internetCCTLD
  continent
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input PersonInputOpt {
  hypi: HypiInputOpt
  names: [PersonNameInputOpt]
  addresses: [AddressInputOpt]
  phones: [PhoneInputOpt]
  dob: DateTime
  gender: Gender
  avatar: ImageInputOpt
  roles: [PairInputOpt]
  preferences: [PairInputOpt]
}

"""Scalar fields defined by StorageCounter"""
enum StorageCounterScalarFields {
  type
  field
  size
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input RequestTemplateInput {
  hypi: HypiInput
  name: String!
  request: String
  response: String
}

"""All fields defined by Hypi"""
enum HypiFields {
  """
  An ID automatically generated by the platform for new objects.
  If provided and the ID does not exist, the provided ID is used instead of a generated one and a new entry is inserted
  If provided and the ID already exists then the existing object is updated.
  """
  id

  """
  When you work with interface fields, Hypi is unable to distinguish which implementation you intend to use
  automatically, you must set this field to the name of the implementation of the interface e.g.
  If creating an AccountPolicy which implements the Policy interface, this field should be set to AccountPolicy
  """
  impl

  """The ISO8601 date of when the object was created"""
  created

  """The ISO8601 date of when the object was last modified"""
  updated

  """
  The ISO8601 date of when the object was trashed (if it is currently trashed, null otherwise)
  """
  trashed

  """The ID of the account which created the object"""
  createdBy

  """The ID of the app instance which created and owns the object"""
  instanceId
  tags
}

"""All fields defined by BruteForceDetectionOptions"""
enum BruteForceDetectionOptionsFields {
  hypi
  maxLoginFailures

  """How long the user ust wait when maxLoginFailures have been reached"""
  waitIncrements
  waitIncrementsUnit

  """
  If login failures occurr too quickly, lock out the user, this sets number of milliseconds that determine "quickly"
  """
  quickLoginCheckMillis

  """How long to wait after a quick failure lock out"""
  minQuickLoginWait
  minQuickLoginWaitUnit

  """max time a user will be locked out for"""
  maxWait
  maxWaitUnit

  """When failure count is reset"""
  failureReset
  failureResetUnit
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input GeoInputOpt {
  envelope: GeoEnvelopeInputOpt
  hypi: HypiInputOpt
  srid: Int
}

input HttpResponseInputOpt {
  headers: Json
  rawPayload: String
  hypi: HypiInputOpt
  status: Int
}

"""JSON scalar"""
scalar Json

type URLAggs {
  path: AggOtherScalar
  queryParams: AggOtherScalar
  port: AggInt
  host: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input AccessTokenGroupByOptions {
  """The field by which to to group the matching data"""
  field: AccessTokenScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""
The precedence of the operations follows BODMAS. https://en.wikipedia.org/wiki/Order_of_operations
For clarity if all fields are specified the precedence is:
1. Divsion
2. Multiplication
3. Subtraction
4. Addition
"""
input MathInputFloat {
  div: Float
  times: Float
  minus: Float
  plus: Float
  hypi: HypiInput!
}

input PasswordInputOpt {
  hypi: HypiInputOpt
  value: String
  expired: Boolean
}

type RealmPolicyAggs {
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""All fields defined by PasswordReminder"""
enum PasswordReminderFields {
  hypi

  """If true the reset code has not yet been used."""
  valid

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code

  """The Account email that needs to be changed"""
  to

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """The Account email that needs to be changed"""
  to_hypi

  """The Account email that needs to be changed"""
  to_value

  """The Account email that needs to be changed"""
  to_type
}

type Person {
  hypi: Hypi
  dob: DateTime
  gender: Gender
  avatar: Image
  names(arcql: String, first: Int, after: String, last: Int, before: String): [PersonName!]!
  addresses(arcql: String, first: Int, after: String, last: Int, before: String): [Address!]
  phones(arcql: String, first: Int, after: String, last: Int, before: String): [Phone!]
  roles(arcql: String, first: Int, after: String, last: Int, before: String): [Pair!]
  preferences(arcql: String, first: Int, after: String, last: Int, before: String): [Pair!]
}

input PasswordReminderInput {
  hypi: HypiInput
  valid: Boolean
  code: String
  to: EmailInput!
  from: String
  subject: String
  htmlMessage: String
  plainTextMessage: String
}

"""Scalar fields defined by NotificationCtx"""
enum NotificationCtxScalarFields {
  """The type that the notification applies to"""
  type
  targetAccount
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input ClientPolicyInput {
  hypi: HypiInput
  clients: [AuthClientInput!]
  name: String!
  logic: AuthLogic
}

"""All fields defined by EmailTemplate"""
enum EmailTemplateFields {
  hypi

  """
   Name of the template being created. The name can contain alpha-numeric characters, digits and next symbols: .-_~
   
  """
  name
  description
  template
  comment
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input RealmPolicyInput {
  hypi: HypiInput
  realms: [RealmLinkInput!]
  name: String!
  logic: AuthLogic
}

"""Scalar fields defined by Person"""
enum PersonScalarFields {
  dob
  gender
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""Scalar fields defined by GroupPolicy"""
enum GroupPolicyScalarFields {
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type URL {
  hypi: Hypi
  path: String!
  queryParams: Json
  port: Int
  host: String
}

input WorkflowConditionalInputOpt {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInputOpt
  repeatN: Int
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
}

"""A list of types on which mutations can be performed on"""
input HypiUpsertInputUnion {
  Message: [MessageInputOpt!]
  User: [UserInputOpt!]
  MessageGroup: [MessageGroupInputOpt!]
  UserStory: [UserStoryInputOpt!]
  Pair: [PairInputOpt!]
  Script: [ScriptInputOpt!]
  RequestTemplate: [RequestTemplateInputOpt!]
  NotificationCtx: [NotificationCtxInputOpt!]
  Notification: [NotificationInputOpt!]
  URL: [URLInputOpt!]
  Currency: [CurrencyInputOpt!]
  Coordinate: [CoordinateInputOpt!]
  GeoEnvelope: [GeoEnvelopeInputOpt!]
  Language: [LanguageInputOpt!]
  Address: [AddressInputOpt!]
  PersonName: [PersonNameInputOpt!]
  Phone: [PhoneInputOpt!]
  Email: [EmailInputOpt!]
  Password: [PasswordInputOpt!]
  RemoteLogin: [RemoteLoginInputOpt!]
  LoginAttempt: [LoginAttemptInputOpt!]
  BruteForceDetectionOptions: [BruteForceDetectionOptionsInputOpt!]
  OAuth2AuthorizedClient: [OAuth2AuthorizedClientInputOpt!]
  AuthClient: [AuthClientInputOpt!]
  ABACPolicy: [ABACPolicyInputOpt!]
  ABACTag: [ABACTagInputOpt!]
  Image: [ImageInputOpt!]
  EmailVerification: [EmailVerificationInputOpt!]
  EmailTemplate: [EmailTemplateInputOpt!]
  EmailSendingAttempt: [EmailSendingAttemptInputOpt!]
  PasswordReminder: [PasswordReminderInputOpt!]
  Webhook: [WebhookInputOpt!]
  WebhookResponse: [WebhookResponseInputOpt!]
  LogMessage: [LogMessageInputOpt!]
  GraphQLRef: [GraphQLRefInputOpt!]
  WorkflowStepData: [WorkflowStepDataInputOpt!]
  WorkflowStep: [WorkflowStepInputOpt!]
  AccessToken: [AccessTokenInputOpt!]
  StorageCounter: [StorageCounterInputOpt!]
  Hypi: [HypiInputOpt!]
  Country: [CountryInputOpt!]
  Account: [AccountInputOpt!]
  Person: [PersonInputOpt!]
  Organisation: [OrganisationInputOpt!]
  OAuthProvider: [OAuthProviderInputOpt!]
  Realm: [RealmInputOpt!]
  Group: [GroupInputOpt!]
  Role: [RoleInputOpt!]
  RolePolicy: [RolePolicyInputOpt!]
  ClientPolicy: [ClientPolicyInputOpt!]
  TimePolicy: [TimePolicyInputOpt!]
  AggregatedPolicy: [AggregatedPolicyInputOpt!]
  GroupPolicy: [GroupPolicyInputOpt!]
  AccountPolicy: [AccountPolicyInputOpt!]
  RealmPolicy: [RealmPolicyInputOpt!]
  RealmLink: [RealmLinkInputOpt!]
  Permission: [PermissionInputOpt!]
  File: [FileInputOpt!]
  Video: [VideoInputOpt!]
  EmailMessage: [EmailMessageInputOpt!]
  Workflow: [WorkflowInputOpt!]
  WorkflowSession: [WorkflowSessionInputOpt!]
  Counter: [CounterInputOpt!]
  Gauge: [GaugeInputOpt!]
  ServerlessResponse: [ServerlessResponseInputOpt!]
}

"""Scalar fields defined by RolePolicy"""
enum RolePolicyScalarFields {
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input AggregatedPolicyInputOpt {
  hypi: HypiInputOpt
  policies: [PolicyInputOpt]
  decisionStrategy: DecisionStrategy
  name: String
  logic: AuthLogic
}

"""All fields defined by AggFloat"""
enum AggFloatFields {
  hypi

  """The value of the aggregated field for each group"""
  groupValues
  avg
  count
  max
  min
  sum
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """The value of the aggregated field for each group"""
  groupValues_hypi

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

"""All numeric fields defined by URL"""
enum URLNumericFields {
  port
}

input WorkflowInput {
  hypi: HypiInput
  name: String!
  steps: [WorkflowStepInput!]
  cronSchedule: String
  execAs: String
  async: Boolean
  parallel: Boolean
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRefInput
  repeatIf: GraphQLRefInput
}

input CountryGroupByOptions {
  """The field by which to to group the matching data"""
  field: CountryScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type AggFloat {
  hypi: Hypi

  """The value of the aggregated field for each group"""
  groupValues: [Pair]
  avg(distinct: Boolean): Float
  count(distinct: Boolean): Int
  max: Float
  min: Float
  sum(distinct: Boolean): Float
}

"""All fields defined by GroupPolicy"""
enum GroupPolicyFields {
  hypi
  name

  """Positive` or `Negative"""
  logic
  groups
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

interface HttpResponse {
  hypi: Hypi
  headers: Json
  status: Int
  rawPayload: String
}

input StorageCounterInput {
  hypi: HypiInput
  type: String!
  field: String!
  size: Int!
}

type LoginAttemptAggs {
  successful: AggOtherScalar
  errorCode: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input EmailVerificationInputOpt {
  hypi: HypiInputOpt
  email: EmailInputOpt
  redirectTo: String
  code: String
  from: String
  subject: String
  templateName: String
  htmlMessage: String
  plainTextMessage: String
  meta: Json
  confirmed: Boolean
}

"""All fields defined by LogMessage"""
enum LogMessageFields {
  hypi
  level
  message
  stackTrace

  """Optional, may not be a stacktrace"""
  releaseId

  """This is optional, we can have system messages that aren't from an app"""
  type

  """
  The name of the GraphQL type that the log is for, this is also optional
  """
  workflow
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input EmailTemplateInput {
  hypi: HypiInput
  name: String
  description: String
  template: String
  comment: String
}

input WebhookResponseInput {
  hypi: HypiInput
  status: Int
  headers: Json
  body: Json
}

"""Scalar fields defined by Webhook"""
enum WebhookScalarFields {
  """
  The name by which this web hook is referenced in the URL, if missing the webhook is only adressable by ID
  """
  name
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_type

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_field

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query_selection
}

"""Scalar fields defined by EmailSendingAttempt"""
enum EmailSendingAttemptScalarFields {
  headers
  body
  status
  statusMessage
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type OAuth2AuthorizedClient {
  hypi: Hypi
  clientRegistrationId: String
  principalName: String
  accessToken: String
  refreshToken: String
}

input NotificationGroupByOptions {
  """The field by which to to group the matching data"""
  field: NotificationScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Coordinate"""
enum CoordinateScalarFields {
  x
  y
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All numeric fields defined by BruteForceDetectionOptions"""
enum BruteForceDetectionOptionsNumericFields {
  maxLoginFailures
  waitIncrements
  quickLoginCheckMillis
  minQuickLoginWait
  maxWait
  failureReset
}

"""Scalar fields defined by URL"""
enum URLScalarFields {
  path
  queryParams
  port
  host
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""Scalar fields defined by LogMessage"""
enum LogMessageScalarFields {
  level
  message
  stackTrace

  """Optional, may not be a stacktrace"""
  releaseId

  """This is optional, we can have system messages that aren't from an app"""
  type

  """
  The name of the GraphQL type that the log is for, this is also optional
  """
  workflow
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by Message"""
enum MessageFields {
  hypi
  from
  to
  in
  text
  replies
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by EmailTemplate"""
enum EmailTemplateScalarFields {
  """
   Name of the template being created. The name can contain alpha-numeric characters, digits and next symbols: .-_~
   
  """
  name
  description
  template
  comment
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type WorkflowStep implements WorkflowAsync & WorkflowTimed & WorkflowRepeatable & WorkflowConditional & WorkflowExecutableAs & WorkflowOrdered {
  hypi: Hypi

  """A name that can be used to reference or trigger this step"""
  name: String

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn: GraphQLRef!
  order: Int!

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs: String
  async: Boolean

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRef
  repeatIf: GraphQLRef
}

input ClientPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: ClientPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""
Published as part of a subscription event's meta data to indicate the nature of the mutation that resulted in the event being triggered.
"""
enum EventType {
  """
   An event type indicating that the cache entry was created.
   
  """
  CREATED

  """
   An event type indicating that the cache entry was updated. i.e. a previous mapping existed
   
  """
  UPDATED

  """
   An event type indicating that the cache entry was removed.
   
  """
  REMOVED

  """
   An event type indicating that the cache entry has expired.
   
  """
  EXPIRED
}

type EmailSendingAttempt {
  hypi: Hypi
  headers: Json
  body: Json
  status: EmailEventType
  statusMessage: String
}

input PersonInput {
  hypi: HypiInput
  names: [PersonNameInput!]!
  addresses: [AddressInput!]
  phones: [PhoneInput!]
  dob: DateTime
  gender: Gender
  avatar: ImageInput
  roles: [PairInput!]
  preferences: [PairInput!]
}

"""All fields defined by NotificationCtx"""
enum NotificationCtxFields {
  hypi

  """The type that the notification applies to"""
  type
  targetAccount
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

interface WorkflowTimed {
  hypi: Hypi

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime: String
}

type StorageCounterAggs {
  type: AggOtherScalar
  field: AggOtherScalar
  size: AggInt
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input AuthClientInputOpt {
  hypi: HypiInputOpt
  name: String
  secret: String
}

input Trigger {
  """
  Executes before the target function. Typically used for validation to prevent execution of a function.
  It MUST have the same arguments as the target function and return Boolean
  """
  before: GraphQLRefInput

  """
  Executed asynchronously after the target. It MUST have the same arguments as the target and can optionally
  have an argument called "hypiResult: T" where T is the type returned by the target
  """
  after: GraphQLRefInput
}

type Counter {
  hypi: Hypi

  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name: String!

  """A human friendly display label for the counter"""
  label: String

  """
  The value of the counter. Semantically this is intended to be monotonically increasing but this is not currently enforced
  See the Gauge type if you want to arbitrarily increase/decrease/set value on a type
  """
  value: Float!
  tags(arcql: String, first: Int, after: String, last: Int, before: String): [String!]
}

type GroupAggs {
  name: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type TimePolicyAggs {
  from: AggOtherScalar
  to: AggOtherScalar
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input AccountPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: AccountPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by Image"""
enum ImageFields {
  hypi
  name
  file
  description
  location
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type GeoEnvelopeAggs {
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by OAuthProvider"""
enum OAuthProviderScalarFields {
  """ instanceId-(hypi.id = registrationId)"""
  clientId
  clientSecret
  clientAuthenticationMethod
  authorizationGrantType
  redirectUriTemplate
  authorizationUri
  tokenUri
  userInfoUri
  userInfoAuthenticationMethod
  userNameAttributeName
  jwkSetUri
  clientName
  hypiSuccessRedirectUri
  hypiFailureRedirectUri
  scopes
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input EmailInput {
  hypi: HypiInput
  value: String!
  type: String
}

input EmailSendingAttemptInput {
  hypi: HypiInput
  headers: Json
  body: Json
  status: EmailEventType
  statusMessage: String
}

"""Scalar fields defined by OAuth2AuthorizedClient"""
enum OAuth2AuthorizedClientScalarFields {
  clientRegistrationId
  principalName
  accessToken
  refreshToken
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""Scalar fields defined by WorkflowStepData"""
enum WorkflowStepDataScalarFields {
  stepName
  stepResult
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by GraphQLRef"""
enum GraphQLRefFields {
  hypi
  type
  field

  """
   If present this is a set of GraphQL fields that will be selected from the results of the function referenced.
   For example if the type returned by field is "T" and T is the object
   type T {
   a: Int
   b: T2
   }
   type T2 {
   c: String
   }
   then this field can be the selection string:
   a b{c}
   i.e. the GraphQL selection you would use if manually selecting fields from T and T2 WITHOUT any curly braces at the start/end - i.e. no enclosing curlies.
   If not provided, the platform will select hypi{id} meaning the result of this function call will have ONLY the hypi.id field
   
  """
  selection
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by File"""
enum FileScalarFields {
  name
  directory
  path
  isDirectory
  status

  """mime type"""
  type
  size
  extension
  isStared
  isSharable
  content
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input AuthClientInput {
  hypi: HypiInput
  name: String!
  secret: String!
}

"""Scalar fields defined by Realm"""
enum RealmScalarFields {
  """
  the name identifying the organisation and becomes the URL by which it is accessed e.g. alpha-corp.hypi.app, where alpha-corp is name
  If not provided one will be automatically generated
  """
  name

  """The name displayed in the user interface"""
  displayName

  """If true users can register without an admin creating their account"""
  allowRegistrations

  """if true users must verify their email before they're allowed to login"""
  verifyEmail
  referrer
  remoteLoginId
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input PermissionInput {
  hypi: HypiInput
  name: String!
  policies: [PolicyInput!]
  decisionStrategy: DecisionStrategy
  type: String!
  scopes: [String!]!
  resource: String
  operationType: OpType!
  operations: [String]!
  includeAllAccounts: Boolean
}

type RemoteLogin {
  hypi: Hypi
  type: String
  email: String
  remoteId: String
  otherAttributes: Json
}

input EmailMessageInput {
  hypi: HypiInput
  from: EmailInput!
  to: [EmailInput!]!
  cc: [EmailInput!]
  bcc: [EmailInput!]
  subject: String!
  template: String
  text: String
  html: String
  attachment: [FileInput!]
  inline: [FileInput!]
  tags: [String]
  deliveryTime: DateTime
  requireTls: Boolean
  skipVerification: Boolean
  headers: Json
  variables: Json
  recipientVariables: Json
  responses: [EmailSendingAttemptInput!]
}

"""Scalar fields defined by GraphQLRef"""
enum GraphQLRefScalarFields {
  type
  field

  """
   If present this is a set of GraphQL fields that will be selected from the results of the function referenced.
   For example if the type returned by field is "T" and T is the object
   type T {
   a: Int
   b: T2
   }
   type T2 {
   c: String
   }
   then this field can be the selection string:
   a b{c}
   i.e. the GraphQL selection you would use if manually selecting fields from T and T2 WITHOUT any curly braces at the start/end - i.e. no enclosing curlies.
   If not provided, the platform will select hypi{id} meaning the result of this function call will have ONLY the hypi.id field
   
  """
  selection
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input ABACPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: ABACPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Permission"""
enum PermissionScalarFields {
  name

  """defines how the policy arrives at a decision, defaults to Unanimous"""
  decisionStrategy

  """The type that this permission applies to"""
  type

  """
   If present then the scopes in this permission will have the given policies applied to this resource.
   This can be used for example to prevent mutation on a resource by a user, group etc
   
  """
  resource

  """Query, Mutation or Subscription"""
  operationType

  """
  If true, this permission grants/denies access to all accounts (including anonymous account)
  """
  includeAllAccounts
  scopes
  operations
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""
Indicates the type should only be evaluated IFF the refrenced function is defined AND executing it returns true.
Any other state results in the evaluation of the conditional e.g.
if evaluateIf is not defined, the conditional is executed.
if evaluateIf is defined but returns anything other than the boolean value true the conditional is executed
"""
interface WorkflowConditional {
  hypi: Hypi
  evaluateIf: GraphQLRef
}

input HypiInputOpt {
  id: ID
  impl: String
  created: DateTime
  updated: DateTime
  trashed: DateTime
  createdBy: ID
  instanceId: String
  tags: [ABACTagInputOpt]
}

"""Scalar fields defined by EmailMessage"""
enum EmailMessageScalarFields {
  subject

  """
  Name of the template to use, if present then the given template is used and text/html etc fields in this message are not used
  """
  template
  text
  html

  """
   Schedule sending in the future
   
  """
  deliveryTime

  """
   If set to True or yes this requires the message only be sent over a TLS connection. If a TLS connection can not be established, we will not deliver the message.
   If set to False or no, we will still try and upgrade the connection, if that fails the message will be delivered over a plaintext SMTP connection.
   
  """
  requireTls

  """
   If set to True or yes, the certificate and hostname will not be verified when trying to establish a TLS connection and Hypi will accept any certificate during delivery.
  
   If set to False or no, Hypi will verify the certificate and hostname. If either one can not be verified, a TLS connection will not be established.
  
   The default is False.
   
  """
  skipVerification

  """
   allows to append a custom MIME header to the message (X-My-Header in this case). For example, h:Reply-To to specify Reply-To address.
   
  """
  headers

  """
   prefix followed by an arbitrary name allows to attach a custom JSON data to the message. See Attaching Data to Messages for more information.
   
  """
  variables

  """
   A valid JSON-encoded dictionary, where key is a plain recipient address and value is a dictionary with variables that can be referenced in the message body.
   
  """
  recipientVariables
  tags
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  from_value
  from_type
  to_value
  to_type
  cc_value
  cc_type
  bcc_value
  bcc_type
}

type Mutation {
  """Removes the link between two objects WITHOUT deleting either object"""
  unlink(
    from: HypiMutationType!
    to: HypiMutationType!
    via: String!
    whereFromID: String!
    andToID: String!

    """The instance ID in which the from ID exists, defaults to this instance"""
    fromInstanceId: String

    """The instance ID in which the to ID exists, defaults to this instance"""
    toInstanceId: String
  ): Boolean!

  """
  Creates a relationship between two existing objects via some field on the source type
  """
  link(
    from: HypiMutationType!
    to: HypiMutationType!
    via: String!
    whereFromID: String!
    andToID: String!

    """The instance ID in which the from ID exists, defaults to this instance"""
    fromInstanceId: String

    """The instance ID in which the to ID exists, defaults to this instance"""
    toInstanceId: String
  ): Boolean!

  """
  Inserts or updates a set of values. To update an existing value provide the hypi.id
  """
  upsert(values: HypiUpsertInputUnion!): [Hypi!]!

  """
  Marks any object that matches the filter as NOT trash if they were previous trashed with the 'trash' function
  """
  untrash(type: HypiMutationType!, arcql: String!): Int!
  math(values: HypiMathType): [Hypi!]!

  """
  Deletes any objects matching the query up to a maximum of 25 (regardless of the first/last arguments if they are bigger than 25)
  """
  delete(
    type: HypiMutationType!
    arcql: String!

    """
    If true this acts as a 'light' cascade by deleting array references to the matched data.Only array references are deleted, one to one references will remain but will return null. A side effect of this behaviour is that if an object is re-created with the same hypi.id then the one to one references that were not cleared will automatically all point to the new object. This takes precedence over the 'enforceReferentialIntegrityOnDeletes' status set on the instance. Defaults to false
    """
    clearArrayReferences: Boolean = false
  ): Int!

  """
  Deletes the given scalar values from the object with the given ID from the field in the specified type
  """
  deleteScalars(
    """The type which contains the field from which you want to delete"""
    type: HypiMutationType!

    """The name of the scalar array field"""
    field: String!

    """The list of values you want to delete"""
    values: [String!]!

    """The query matching the objects to delete the values from"""
    arcql: String!
  ): Int!

  """
  Marks any object that matches the filter as trash. Those objects will not be returned in get or find queries unless they're marked as not trash later. Maximum of 25 will be trashed (regardless of the first/last arguments if they are bigger than 25)
  """
  trash(type: HypiMutationType!, arcql: String!): Int!

  """
  Used to perform registration of a new realm. Normally only Hypi directly calls this
  """
  createRealm(value: RealmInput!): Hypi

  """
  Creates a new account which can be used to login and perform various actions in a Realm
  """
  createAccount(value: AccountInput!): Hypi
}

"""All fields defined by Coordinate"""
enum CoordinateFields {
  hypi
  x
  y
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type File {
  hypi: Hypi
  name: String!
  directory: String!
  path: String!
  isDirectory: Boolean!
  status: FileStatus
  url: URL

  """mime type"""
  type: String
  size: Long
  extension: String
  isStared: Boolean
  isSharable: Boolean
  content: String
  children(arcql: String, first: Int, after: String, last: Int, before: String): [File!]
}

input ScriptGroupByOptions {
  """The field by which to to group the matching data"""
  field: ScriptScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type RealmAggs {
  name: AggOtherScalar
  displayName: AggOtherScalar
  allowRegistrations: AggOtherScalar
  verifyEmail: AggOtherScalar
  referrer: AggOtherScalar
  remoteLoginId: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input ABACTagInput {
  hypi: HypiInput
  key: String!
  value: String
}

"""
Any scalar. DO NOT USE except in extremely rare cases where it is unavoidable to do so
"""
scalar Any

input RealmPolicyInputOpt {
  hypi: HypiInputOpt
  realms: [RealmLinkInputOpt]
  name: String
  logic: AuthLogic
}

interface WorkflowParallel {
  hypi: Hypi

  """
  If present AND true, all steps in this block are executed at the same time.
  """
  parallel: Boolean
}

type EmailVerificationAggs {
  redirectTo: AggOtherScalar
  code: AggOtherScalar
  from: AggOtherScalar
  subject: AggOtherScalar
  templateName: AggOtherScalar
  htmlMessage: AggOtherScalar
  plainTextMessage: AggOtherScalar
  meta: AggOtherScalar
  confirmed: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  email_value: AggOtherScalar
  email_type: AggOtherScalar
}

"""Scalar fields defined by GeoEnvelope"""
enum GeoEnvelopeScalarFields {
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type Phone {
  hypi: Hypi
  number: String!
  country: Country
  code: String
}

input RealmLinkGroupByOptions {
  """The field by which to to group the matching data"""
  field: RealmLinkScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Pair"""
enum PairScalarFields {
  key
  value
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by AccessToken"""
enum AccessTokenFields {
  hypi
  sessionToken
  sessionExpires
  errorCode
  errorMsg
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type GraphQLRefAggs {
  type: AggOtherScalar
  field: AggOtherScalar
  selection: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by AccessToken"""
enum AccessTokenScalarFields {
  sessionToken
  sessionExpires
  errorCode
  errorMsg
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type ABACTagAggs {
  key: AggOtherScalar
  value: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

interface WorkflowExecutableAs {
  hypi: Hypi

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs: String
}

input LoginAttemptInputOpt {
  hypi: HypiInputOpt
  successful: Boolean
  errorCode: String
}

"""Scalar fields defined by RemoteLogin"""
enum RemoteLoginScalarFields {
  type
  email
  remoteId
  otherAttributes
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All numeric fields defined by WorkflowStep"""
enum WorkflowStepNumericFields {
  order
  repeatN
}

"""All fields defined by EmailSendingAttempt"""
enum EmailSendingAttemptFields {
  hypi
  headers
  body
  status
  statusMessage
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by LoginAttempt"""
enum LoginAttemptScalarFields {
  successful
  errorCode
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input MessageGroupInput {
  hypi: HypiInput
  members: [UserInput!]
}

input WebhookInputOpt {
  hypi: HypiInputOpt
  name: String
  as: AccountInputOpt
  query: GraphQLRefInputOpt
}

"""All fields defined by Email"""
enum EmailFields {
  hypi
  value
  type
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input StorageCounterGroupByOptions {
  """The field by which to to group the matching data"""
  field: StorageCounterScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All numeric fields defined by StorageCounter"""
enum StorageCounterNumericFields {
  size
}

type GeoEnvelope {
  hypi: Hypi
  p1: Coordinate!
  p2: Coordinate!
}

type UserAggs {
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input WebhookGroupByOptions {
  """The field by which to to group the matching data"""
  field: WebhookScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type WorkflowStepDataAggs {
  stepName: AggOtherScalar
  stepResult: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input ImageInputOpt {
  hypi: HypiInputOpt
  name: String
  file: FileInputOpt
  description: String
  location: GeoInputOpt
}

"""The types of GraphQL queries"""
enum OpType {
  Query
  Mutation
  Subscription
}

type GaugeAggs {
  name: AggOtherScalar
  label: AggOtherScalar
  value: AggFloat
  tags: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Creates a reference to a GraphQL function in an app instance."""
type GraphQLRef {
  hypi: Hypi
  type: OpType!
  field: String!

  """
   If present this is a set of GraphQL fields that will be selected from the results of the function referenced.
   For example if the type returned by field is "T" and T is the object
   type T {
   a: Int
   b: T2
   }
   type T2 {
   c: String
   }
   then this field can be the selection string:
   a b{c}
   i.e. the GraphQL selection you would use if manually selecting fields from T and T2 WITHOUT any curly braces at the start/end - i.e. no enclosing curlies.
   If not provided, the platform will select hypi{id} meaning the result of this function call will have ONLY the hypi.id field
   
  """
  selection: String
}

input PasswordReminderGroupByOptions {
  """The field by which to to group the matching data"""
  field: PasswordReminderScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input FileInputOpt {
  hypi: HypiInputOpt
  name: String
  directory: String
  path: String
  isDirectory: Boolean
  status: FileStatus
  url: URLInputOpt
  children: [FileInputOpt]
  type: String
  size: Long
  extension: String
  isStared: Boolean
  isSharable: Boolean
  content: String
}

type ClientPolicy implements Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  clients(arcql: String, first: Int, after: String, last: Int, before: String): [AuthClient!]
}

type LogMessageAggs {
  level: AggOtherScalar
  message: AggOtherScalar
  stackTrace: AggOtherScalar
  releaseId: AggOtherScalar
  type: AggOtherScalar
  workflow: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by RealmPolicy"""
enum RealmPolicyScalarFields {
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input GaugeGroupByOptions {
  """The field by which to to group the matching data"""
  field: GaugeScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by AggInt"""
enum AggIntFields {
  hypi

  """The value of the aggregated field for each group"""
  groupValues
  avg
  count
  max
  min
  sum
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """The value of the aggregated field for each group"""
  groupValues_hypi

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

input WorkflowAsyncInputOpt {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInputOpt
  repeatN: Int
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
}

input WorkflowTimedInputOpt {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInputOpt
  repeatN: Int
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  repeatIf: GraphQLRefInputOpt
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInputOpt
  evaluateIf: GraphQLRefInputOpt
  order: Int
}

"""All numeric fields defined by WebhookResponse"""
enum WebhookResponseNumericFields {
  status
}

input ImageGroupByOptions {
  """The field by which to to group the matching data"""
  field: ImageScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by ABACTag"""
enum ABACTagScalarFields {
  key

  """If provided then policy assertion can be made against it"""
  value
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input PairInput {
  hypi: HypiInput
  key: String
  value: String
}

type Currency {
  hypi: Hypi
  name: String!
  code: String!
  symbol: String!
}

type Permission {
  hypi: Hypi
  name: String!

  """defines how the policy arrives at a decision, defaults to Unanimous"""
  decisionStrategy: DecisionStrategy

  """The type that this permission applies to"""
  type: String!

  """
   If present then the scopes in this permission will have the given policies applied to this resource.
   This can be used for example to prevent mutation on a resource by a user, group etc
   
  """
  resource: String

  """Query, Mutation or Subscription"""
  operationType: OpType!

  """
  If true, this permission grants/denies access to all accounts (including anonymous account)
  """
  includeAllAccounts: Boolean
  policies(arcql: String, first: Int, after: String, last: Int, before: String): [Policy!]
  scopes(arcql: String, first: Int, after: String, last: Int, before: String): [String!]!
  operations(arcql: String, first: Int, after: String, last: Int, before: String): [String]!
}

"""Time units used in Auth* types"""
enum TimeUnit {
  SECONDS
  MINUTES
  HOURS
  DAYS
}

"""All fields defined by TimePolicy"""
enum TimePolicyFields {
  hypi

  """
  (yyyy-MM-dd hh:mm:ss) can be used for example to ensure a file is not viewable before the given date
  """
  from

  """can be used to ensure a file is not viewable after a given date"""
  to
  name

  """Positive` or `Negative"""
  logic
  clients
  roles
  groups
  accounts
  realms
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Email related"""
type EmailVerification {
  hypi: Hypi
  email: Email!

  """
  After the link is clicked from the email, redirect the browser to this URL passing a token in the URL i.e. token=jwt.token.here which can be used get the value in the meta field
  """
  redirectTo: String!

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code: String

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from: String

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject: String
  templateName: String

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage: String

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage: String

  """
  Any additional meta data you want to store. For example, you could collect all of the information needed to create the Account
  """
  meta: Json

  """Set by system, cannot be provided"""
  confirmed: Boolean
}

input WebhookResponseMaths {
  status: MathInputInt
}

"""All fields defined by Account"""
enum AccountFields {
  hypi
  verified
  enabled
  username
  password
  owner
  emails
  phones
  groups
  roles
  attempts
  remoteLogins
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""The event field of HypiEvent will be one of these"""
enum EmailEventType {
  """Hypi sent the email and it was accepted by the recipient email server."""
  delivered

  """
   Hypi could not deliver the email to the recipient email server.
   
  """
  failed
}

"""Scalar fields defined by Address"""
enum AddressScalarFields {
  door
  street
  town
  county
  city
  postCode

  """  country: Country"""
  from
  to
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input WorkflowStepGroupByOptions {
  """The field by which to to group the matching data"""
  field: WorkflowStepScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Group"""
enum GroupScalarFields {
  """
  A unique name identifying this group, implicitly sets the path of the group to /<name> whihc can be referenced in wild card permissions
  """
  name
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input UserStoryInputOpt {
  hypi: HypiInputOpt
  text: String
  img: String
}

"""
A workflow defines a sequence of steps that execute in a defined order (you set the order field on each step).
It is one way in which Hypi allows you to do composition, so similarities can be drawn to function composition with some specifics thrown in for Hypi and GraphQL.

If two steps have the same order their execution order is undefined with relation to each other.
Every step has a GraphQL function that is executed for that step.

When a Workflow is executed it creates a WorkflowSession. The result of each step in the workflow is added to the session.

The first step can have any parameters you want. For the other steps there are some rules that define how the system maps
parameters to the function in these steps. These rules are:

1. Any step (including the first step) can have a parameter "params: Json" i.e. name = params and type is Json.
This params is a map of the arguments passed to the first function in the Workflow. For example if the function was defined as
step1(a: Int, b: Json, c: MyType): T
in this case the "params" Json object would be have the fields a, b and c set to the values the function was executed with.
Normally, this is used in the first step but can be used in any step that wants access to this data.

2. Any step can have a parameter "session: WorkflowSession" - this is the current workflow's session and contains the results of all steps before the current one.
You can identify the results for a specific step by finding the result using the step's name in the session's data array.

3. Except the first step, a parameter "previous: T" where T is the result type of the previous step can be used.
In this case, the platform will use the output of the previous function for this parameter.
Note that if the type is not the same as the last step's output type then the workflow will fail if the field is not optional.
If the field is optional then the platform will not provide it and it would therefore be null if you try to use it.

4. Except the first step, pass-through is possible. This is where the parameters from the first step are passed through
to other steps by name and type. i.e. given
step1(a: Int, b: String): String
step2(a: Int): ID
In this case, the variable "a" in both step1 and step2 will have the same value that step1 was executed with.
Incidentally, this is the same as getting "a" from the "params" Json.
"""
type Workflow implements WorkflowAsync & WorkflowTimed & WorkflowRepeatable & WorkflowConditional & WorkflowParallel & WorkflowExecutableAs {
  hypi: Hypi
  name: String!

  """
  If present, this is a cron schedule to automatically execute this Workflow
  The syntax as defined at https://www.manpagez.com/man/5/crontab/
  NOTE: The special strings @hourly, @daily etc are NOT supported
  """
  cronSchedule: String

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs: String
  async: Boolean

  """
  If present AND true, all steps in this block are executed at the same time.
  """
  parallel: Boolean

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRef
  repeatIf: GraphQLRef
  steps(arcql: String, first: Int, after: String, last: Int, before: String): [WorkflowStep!]
}

input URLGroupByOptions {
  """The field by which to to group the matching data"""
  field: URLScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input WorkflowAsyncInput {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInput
  repeatN: Int
  steps: [WorkflowStepInput!]
  cronSchedule: String
  repeatIf: GraphQLRefInput
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int
}

"""All fields defined by RequestTemplate"""
enum RequestTemplateFields {
  hypi
  name
  request
  response
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input BruteForceDetectionOptionsMaths {
  maxLoginFailures: MathInputInt
  waitIncrements: MathInputInt
  quickLoginCheckMillis: MathInputInt
  minQuickLoginWait: MathInputInt
  maxWait: MathInputInt
  failureReset: MathInputInt
}

"""All fields defined by WorkflowStepData"""
enum WorkflowStepDataFields {
  hypi
  stepName
  stepResult
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by PersonName"""
enum PersonNameScalarFields {
  title
  firstName
  lastName
  from
  to
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type LanguageAggs {
  family: AggOtherScalar
  isoName: AggOtherScalar
  nativeName: AggOtherScalar
  iso6391Code: AggOtherScalar
  iso6392TCode: AggOtherScalar
  iso6392BCode: AggOtherScalar
  iso6393Code: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input OrganisationGroupByOptions {
  """The field by which to to group the matching data"""
  field: OrganisationScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Defines a collection for subjects, roles, policies and permissions."""
type Group {
  hypi: Hypi

  """
  A unique name identifying this group, implicitly sets the path of the group to /<name> whihc can be referenced in wild card permissions
  """
  name: String!
  accounts(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]
  children(arcql: String, first: Int, after: String, last: Int, before: String): [Group!]
  organisations(arcql: String, first: Int, after: String, last: Int, before: String): [Organisation!]
}

input AuthClientGroupByOptions {
  """The field by which to to group the matching data"""
  field: AuthClientScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input LanguageGroupByOptions {
  """The field by which to to group the matching data"""
  field: LanguageScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input NotificationInputOpt {
  hypi: HypiInputOpt
  message: String
  ctx: NotificationCtxInputOpt
}

"""
A client defines an agent that acts on behalf of a user/subject.
Currently implicitly created by Hypi.
"""
type AuthClient {
  hypi: Hypi
  name: String!
  secret: String!
}

"""Scalar fields defined by AuthClient"""
enum AuthClientScalarFields {
  name
  secret
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by ServerlessResponse"""
enum ServerlessResponseFields {
  hypi
  path
  headers
  multiPart
  method
  chunked
  queryString
  body
  cookies
  files
  attributes
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by WorkflowStep"""
enum WorkflowStepFields {
  hypi

  """A name that can be used to reference or trigger this step"""
  name

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn
  order

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs
  async

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime
  repeatN
  evaluateIf
  repeatIf
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_hypi

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_type

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_field

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_selection
  evaluateIf_hypi
  evaluateIf_type
  evaluateIf_field
  evaluateIf_selection
  repeatIf_hypi
  repeatIf_type
  repeatIf_field
  repeatIf_selection
}

input CounterInputOpt {
  hypi: HypiInputOpt
  name: String
  label: String
  tags: [String]
  value: Float
}

input WorkflowStepInputOpt {
  hypi: HypiInputOpt
  name: String
  fn: GraphQLRefInputOpt
  order: Int
  execAs: String
  async: Boolean
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRefInputOpt
  repeatIf: GraphQLRefInputOpt
}

input MessageGroupInputOpt {
  hypi: HypiInputOpt
  members: [UserInputOpt]
}

"""All fields defined by GeoEnvelope"""
enum GeoEnvelopeFields {
  hypi
  p1
  p2
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input PasswordGroupByOptions {
  """The field by which to to group the matching data"""
  field: PasswordScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type ABACTag {
  hypi: Hypi
  key: String!

  """If provided then policy assertion can be made against it"""
  value: String
}

"""All fields defined by Notification"""
enum NotificationFields {
  hypi
  message
  ctx
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
  ctx_hypi
  ctx_type
  ctx_targetAccount
}

type HypiFilterConnection {
  hypi: Hypi
  edges: [HypiResultEdge!]
  pageInfo: PageInfo!
}

input RequestTemplateInputOpt {
  hypi: HypiInputOpt
  name: String
  request: String
  response: String
}

"""A list of types on which aggregation queries can be executed"""
type HypiAggregationType {
  message(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): MessageAggs
  messageWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [MessageGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [MessageAggs]
  user(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): UserAggs
  userWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [UserGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [UserAggs]
  messageGroup(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): MessageGroupAggs
  messageGroupWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [MessageGroupGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [MessageGroupAggs]
  userStory(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): UserStoryAggs
  userStoryWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [UserStoryGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [UserStoryAggs]
  script(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ScriptAggs
  scriptWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ScriptGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ScriptAggs]
  requestTemplate(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RequestTemplateAggs
  requestTemplateWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RequestTemplateGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RequestTemplateAggs]
  notificationCtx(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): NotificationCtxAggs
  notificationCtxWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [NotificationCtxGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [NotificationCtxAggs]
  notification(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): NotificationAggs
  notificationWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [NotificationGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [NotificationAggs]
  uRL(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): URLAggs
  uRLWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [URLGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [URLAggs]
  currency(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): CurrencyAggs
  currencyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [CurrencyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [CurrencyAggs]
  coordinate(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): CoordinateAggs
  coordinateWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [CoordinateGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [CoordinateAggs]
  geoEnvelope(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): GeoEnvelopeAggs
  geoEnvelopeWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [GeoEnvelopeGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [GeoEnvelopeAggs]
  language(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): LanguageAggs
  languageWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [LanguageGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [LanguageAggs]
  address(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AddressAggs
  addressWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AddressGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AddressAggs]
  personName(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PersonNameAggs
  personNameWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PersonNameGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PersonNameAggs]
  phone(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PhoneAggs
  phoneWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PhoneGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PhoneAggs]
  email(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): EmailAggs
  emailWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [EmailGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [EmailAggs]
  password(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PasswordAggs
  passwordWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PasswordGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PasswordAggs]
  remoteLogin(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RemoteLoginAggs
  remoteLoginWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RemoteLoginGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RemoteLoginAggs]
  loginAttempt(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): LoginAttemptAggs
  loginAttemptWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [LoginAttemptGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [LoginAttemptAggs]
  bruteForceDetectionOptions(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): BruteForceDetectionOptionsAggs
  bruteForceDetectionOptionsWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [BruteForceDetectionOptionsGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [BruteForceDetectionOptionsAggs]
  oAuth2AuthorizedClient(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): OAuth2AuthorizedClientAggs
  oAuth2AuthorizedClientWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [OAuth2AuthorizedClientGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [OAuth2AuthorizedClientAggs]
  authClient(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AuthClientAggs
  authClientWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AuthClientGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AuthClientAggs]
  aBACPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ABACPolicyAggs
  aBACPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ABACPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ABACPolicyAggs]
  aBACTag(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ABACTagAggs
  aBACTagWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ABACTagGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ABACTagAggs]
  image(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ImageAggs
  imageWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ImageGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ImageAggs]
  emailVerification(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): EmailVerificationAggs
  emailVerificationWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [EmailVerificationGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [EmailVerificationAggs]
  emailTemplate(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): EmailTemplateAggs
  emailTemplateWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [EmailTemplateGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [EmailTemplateAggs]
  emailSendingAttempt(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): EmailSendingAttemptAggs
  emailSendingAttemptWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [EmailSendingAttemptGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [EmailSendingAttemptAggs]
  passwordReminder(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PasswordReminderAggs
  passwordReminderWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PasswordReminderGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PasswordReminderAggs]
  webhook(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WebhookAggs
  webhookWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WebhookGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WebhookAggs]
  webhookResponse(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WebhookResponseAggs
  webhookResponseWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WebhookResponseGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WebhookResponseAggs]
  logMessage(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): LogMessageAggs
  logMessageWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [LogMessageGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [LogMessageAggs]
  graphQLRef(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): GraphQLRefAggs
  graphQLRefWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [GraphQLRefGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [GraphQLRefAggs]
  workflowStepData(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WorkflowStepDataAggs
  workflowStepDataWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WorkflowStepDataGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WorkflowStepDataAggs]
  workflowStep(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WorkflowStepAggs
  workflowStepWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WorkflowStepGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WorkflowStepAggs]
  accessToken(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AccessTokenAggs
  accessTokenWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AccessTokenGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AccessTokenAggs]
  storageCounter(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): StorageCounterAggs
  storageCounterWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [StorageCounterGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [StorageCounterAggs]
  country(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): CountryAggs
  countryWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [CountryGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [CountryAggs]
  account(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AccountAggs
  accountWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AccountGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AccountAggs]
  person(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PersonAggs
  personWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PersonGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PersonAggs]
  organisation(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): OrganisationAggs
  organisationWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [OrganisationGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [OrganisationAggs]
  oAuthProvider(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): OAuthProviderAggs
  oAuthProviderWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [OAuthProviderGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [OAuthProviderAggs]
  realm(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RealmAggs
  realmWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RealmGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RealmAggs]
  group(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): GroupAggs
  groupWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [GroupGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [GroupAggs]
  role(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RoleAggs
  roleWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RoleGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RoleAggs]
  rolePolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RolePolicyAggs
  rolePolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RolePolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RolePolicyAggs]
  clientPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ClientPolicyAggs
  clientPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ClientPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ClientPolicyAggs]
  timePolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): TimePolicyAggs
  timePolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [TimePolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [TimePolicyAggs]
  aggregatedPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AggregatedPolicyAggs
  aggregatedPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AggregatedPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AggregatedPolicyAggs]
  groupPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): GroupPolicyAggs
  groupPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [GroupPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [GroupPolicyAggs]
  accountPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): AccountPolicyAggs
  accountPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [AccountPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [AccountPolicyAggs]
  realmPolicy(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RealmPolicyAggs
  realmPolicyWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RealmPolicyGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RealmPolicyAggs]
  realmLink(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): RealmLinkAggs
  realmLinkWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [RealmLinkGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [RealmLinkAggs]
  permission(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): PermissionAggs
  permissionWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [PermissionGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [PermissionAggs]
  file(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): FileAggs
  fileWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [FileGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [FileAggs]
  video(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): VideoAggs
  videoWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [VideoGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [VideoAggs]
  emailMessage(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): EmailMessageAggs
  emailMessageWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [EmailMessageGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [EmailMessageAggs]
  workflow(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WorkflowAggs
  workflowWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WorkflowGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WorkflowAggs]
  workflowSession(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): WorkflowSessionAggs
  workflowSessionWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [WorkflowSessionGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [WorkflowSessionAggs]
  counter(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): CounterAggs
  counterWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [CounterGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [CounterAggs]
  gauge(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): GaugeAggs
  gaugeWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [GaugeGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [GaugeAggs]
  serverlessResponse(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String
  ): ServerlessResponseAggs
  serverlessResponseWith(
    """
    Filters rows before the they are summarised into groups by the GROUP BY clause
    """
    where: String

    """
    Combines rows into groups based on matching values in specified columns. One row is returned for each group.
    """
    groupBy: [ServerlessResponseGroupByOptions!]!

    """Filters rows after the results are grouped"""
    having: String
    first: Int
    after: String
    last: Int
    before: String
    includeTrashed: Boolean

    """
    The page number to offset results by. This should be used in combination with after (does not work with before). It can be used by itself but carries some performance impact as a result we limit this to a max of 25
    """
    page: Int
  ): [ServerlessResponseAggs]
}

input RoleGroupByOptions {
  """The field by which to to group the matching data"""
  field: RoleScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input CoordinateGroupByOptions {
  """The field by which to to group the matching data"""
  field: CoordinateScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by BruteForceDetectionOptions"""
enum BruteForceDetectionOptionsScalarFields {
  maxLoginFailures

  """How long the user ust wait when maxLoginFailures have been reached"""
  waitIncrements
  waitIncrementsUnit

  """
  If login failures occurr too quickly, lock out the user, this sets number of milliseconds that determine "quickly"
  """
  quickLoginCheckMillis

  """How long to wait after a quick failure lock out"""
  minQuickLoginWait
  minQuickLoginWaitUnit

  """max time a user will be locked out for"""
  maxWait
  maxWaitUnit

  """When failure count is reset"""
  failureReset
  failureResetUnit
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input CounterInput {
  hypi: HypiInput
  name: String!
  label: String
  tags: [String!]
  value: Float!
}

enum AuthorizationGrantType {
  authorization_code
  implicit
  refresh_token
  client_credentials
}

"""All fields defined by URL"""
enum URLFields {
  hypi
  path
  queryParams
  port
  host
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input EmailMessageInputOpt {
  hypi: HypiInputOpt
  from: EmailInputOpt
  to: [EmailInputOpt]
  cc: [EmailInputOpt]
  bcc: [EmailInputOpt]
  subject: String
  template: String
  text: String
  html: String
  attachment: [FileInputOpt]
  inline: [FileInputOpt]
  tags: [String]
  deliveryTime: DateTime
  requireTls: Boolean
  skipVerification: Boolean
  headers: Json
  variables: Json
  recipientVariables: Json
  responses: [EmailSendingAttemptInputOpt]
}

"""
Defines a web hook that can be used to trigger Hypi GraphQL functions on a given app.
The account specified in the web hook must have access to the app/instance and must be authorised to call the functions specified.
"""
type Webhook {
  hypi: Hypi

  """
  The name by which this web hook is referenced in the URL, if missing the webhook is only adressable by ID
  """
  name: String

  """
   Defaults to the account creating the Webhook.
   Hypi will generate an authorisation token automatically for the account when the web hook is triggered.
   This token will then be used to execute the triggers in the web hook (query or mutation).
   For security an account should be created specifically for invoking web hooks and an AccountPolicy should be created that grants access only to the specified functions or otherwise limit the scope of what the account can do.
   
  """
  as: Account

  """
   This refers to a GraphQL function.
   The function must have a graphql argument defined as `(payload: WebhookPayload): WebhookPayload`
   The function can trigger a workflow or operate on the payload itself.
   
  """
  query: GraphQLRef!
}

input OrganisationInput {
  hypi: HypiInput
  name: String!
  logo: ImageInput
  addresses: [AddressInput!]
  incorporated: DateTime
  phones: [PhoneInput!]
  emails: [EmailInput!]
  members: [AccountInput!]
  subsidiaries: [OrganisationInput!]
}

input CoordinateMaths {
  x: MathInputFloat
  y: MathInputFloat
}

input AccessTokenInput {
  hypi: HypiInput
  sessionToken: String
  sessionExpires: Long
  errorCode: String
  errorMsg: String
}

"""All fields defined by Pair"""
enum PairFields {
  hypi
  key
  value
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

enum HashAlgorithm {
  SHA3
  BCRYPT

  """
   PKCS i.e. Public Key Cryptography Standards. When used Hypi will encrypt/decrypt the contents using a public/private key pair.
   So the data is encrypted at rest and decrypted whilst in use.
   See https://en.wikipedia.org/wiki/PKCS, specifically Password-based Encryption Standard
   
  """
  PKCS5
}

enum FileStatus {
  UPLOADED
  PROCESSING
  PENDING_APPROVAL
  DISABLED
  AVAILABLE
  DELETED
  UNAVAILABLE
}

"""
A row is created for every resource. It cannot be created or modified by end users.
The ID of each entry is a hash of the resource ID, type and field.
When the resource is deleted, the entry is deleted.
"""
type StorageCounter {
  hypi: Hypi
  type: String!
  field: String!
  size: Int!
}

input ServerlessResponseInput {
  hypi: HypiInput
  path: String
  headers: Json
  multiPart: Boolean
  method: String
  files: [FileInput]
  chunked: Boolean
  attributes: [String]
  queryString: Json
  body: Json
  cookies: Json
}

"""All fields defined by Gauge"""
enum GaugeFields {
  hypi

  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name

  """A human friendly display label for the counter"""
  label

  """
  The current value of this gauge, set, increase or decrease as you see fit
  """
  value
  tags
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input RoleInput {
  hypi: HypiInput
  name: String!
  accounts: [AccountInput!]
}

input TimePolicyInput {
  hypi: HypiInput
  from: DateTime
  to: DateTime
  clients: [AuthClientInput!]
  roles: [RoleInput!]
  groups: [GroupInput!]
  accounts: [AccountInput!]
  realms: [RealmLinkInput!]
  name: String!
  logic: AuthLogic
}

input WorkflowInputOpt {
  hypi: HypiInputOpt
  name: String
  steps: [WorkflowStepInputOpt]
  cronSchedule: String
  execAs: String
  async: Boolean
  parallel: Boolean
  maxExecutionTime: String
  repeatN: Int
  evaluateIf: GraphQLRefInputOpt
  repeatIf: GraphQLRefInputOpt
}

"""
The decision strategy dictates how the policies associated with a given permission are evaluated and how a final decision is obtained.
'Affirmative' means that at least one policy must evaluate to a positive decision in order for the final decision to be also positive.
'Unanimous' means that all policies must evaluate to a positive decision in order for the final decision to be also positive.
'Consensus' means that the number of positive decisions must be greater than the number of negative decisions.
If the number of positive and negative is the same, the final decision will be negative.
"""
enum DecisionStrategy {
  """
  There MUST be at least one policy AND all policies listed must be positive for this policy to result in a positive decision
  """
  Unanimous

  """
  at least one policy listed must be positive for this policy to result in a positive decision
  """
  Affirmative

  """
  The number of policies that are positive must be greater than those that are negative e.g. if 5 policies are included, at least 3 must be positive for this policy to be positive
  """
  Consensus
}

"""Scalar fields defined by MessageGroup"""
enum MessageGroupScalarFields {
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by Counter"""
enum CounterFields {
  hypi

  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name

  """A human friendly display label for the counter"""
  label

  """
  The value of the counter. Semantically this is intended to be monotonically increasing but this is not currently enforced
  See the Gauge type if you want to arbitrarily increase/decrease/set value on a type
  """
  value
  tags
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input CurrencyInput {
  hypi: HypiInput
  name: String!
  code: String!
  symbol: String!
}

"""All fields defined by Role"""
enum RoleFields {
  hypi
  name
  accounts
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input GroupPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: GroupPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input GroupInput {
  hypi: HypiInput
  name: String!
  accounts: [AccountInput!]
  children: [GroupInput!]
  organisations: [OrganisationInput!]
}

"""All fields defined by Person"""
enum PersonFields {
  hypi
  dob
  gender
  avatar
  names
  addresses
  phones
  roles
  preferences
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type RoleAggs {
  name: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input NotificationInput {
  hypi: HypiInput
  message: String
  ctx: NotificationCtxInput
}

input MessageGroupByOptions {
  """The field by which to to group the matching data"""
  field: MessageScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Counter"""
enum CounterScalarFields {
  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name

  """A human friendly display label for the counter"""
  label

  """
  The value of the counter. Semantically this is intended to be monotonically increasing but this is not currently enforced
  See the Gauge type if you want to arbitrarily increase/decrease/set value on a type
  """
  value
  tags
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input PersonGroupByOptions {
  """The field by which to to group the matching data"""
  field: PersonScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type PermissionDescription {
  hypi: Hypi
  roles: [Role!]
  groups: [Group!]
  organisations: [Organisation!]
  realms: [Realm!]
  permissions(after: String, limit: Int = 25, type: String, resources: [String!]): [Permission]
}

input WorkflowExecutableAsInput {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInput
  repeatN: Int
  steps: [WorkflowStepInput!]
  cronSchedule: String
  repeatIf: GraphQLRefInput
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int
}

type AggOtherScalar {
  hypi: Hypi

  """The value of the aggregated field for each group"""
  groupValues: [Pair]
  count(distinct: Boolean): Int
}

"""Long type"""
scalar Long

"""
Defines a Hypi template that can be parameterised
https://documentation.Hypi.com/en/latest/api-templates.html#store-new-template
"""
type EmailTemplate {
  hypi: Hypi

  """
   Name of the template being created. The name can contain alpha-numeric characters, digits and next symbols: .-_~
   
  """
  name: String
  description: String
  template: String
  comment: String
}

"""All numeric fields defined by Counter"""
enum CounterNumericFields {
  value
}

input PersonNameInput {
  hypi: HypiInput
  title: String
  firstName: String
  lastName: String
  from: DateTime
  to: DateTime
}

"""All fields defined by AggOtherScalar"""
enum AggOtherScalarFields {
  hypi

  """The value of the aggregated field for each group"""
  groupValues
  count
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags

  """The value of the aggregated field for each group"""
  groupValues_hypi

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

type WorkflowSessionAggs {
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""All fields defined by RemoteLogin"""
enum RemoteLoginFields {
  hypi
  type
  email
  remoteId
  otherAttributes
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""
An object injected into ALL types as the field "hypi"
"""
type Hypi {
  """
  An ID automatically generated by the platform for new objects.
  If provided and the ID does not exist, the provided ID is used instead of a generated one and a new entry is inserted
  If provided and the ID already exists then the existing object is updated.
  """
  id: ID

  """
  When you work with interface fields, Hypi is unable to distinguish which implementation you intend to use
  automatically, you must set this field to the name of the implementation of the interface e.g.
  If creating an AccountPolicy which implements the Policy interface, this field should be set to AccountPolicy
  """
  impl: String

  """The ISO8601 date of when the object was created"""
  created: DateTime

  """The ISO8601 date of when the object was last modified"""
  updated: DateTime

  """
  The ISO8601 date of when the object was trashed (if it is currently trashed, null otherwise)
  """
  trashed: DateTime

  """The ID of the account which created the object"""
  createdBy: ID

  """The ID of the app instance which created and owns the object"""
  instanceId: String
  tags(arcql: String, first: Int, after: String, last: Int, before: String): [ABACTag!]
}

type AccessTokenAggs {
  sessionToken: AggOtherScalar
  sessionExpires: AggOtherScalar
  errorCode: AggOtherScalar
  errorMsg: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input RoleInputOpt {
  hypi: HypiInputOpt
  name: String
  accounts: [AccountInputOpt]
}

type PersonNameAggs {
  title: AggOtherScalar
  firstName: AggOtherScalar
  lastName: AggOtherScalar
  from: AggOtherScalar
  to: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type TimePolicy implements Policy {
  hypi: Hypi

  """
  (yyyy-MM-dd hh:mm:ss) can be used for example to ensure a file is not viewable before the given date
  """
  from: DateTime

  """can be used to ensure a file is not viewable after a given date"""
  to: DateTime
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  clients(arcql: String, first: Int, after: String, last: Int, before: String): [AuthClient!]
  roles(arcql: String, first: Int, after: String, last: Int, before: String): [Role!]
  groups(arcql: String, first: Int, after: String, last: Int, before: String): [Group!]
  accounts(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]
  realms(arcql: String, first: Int, after: String, last: Int, before: String): [RealmLink!]
}

"""Scalar fields defined by Currency"""
enum CurrencyScalarFields {
  name
  code
  symbol
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type CurrencyAggs {
  name: AggOtherScalar
  code: AggOtherScalar
  symbol: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input URLInput {
  hypi: HypiInput
  path: String!
  queryParams: Json
  port: Int
  host: String
}

type EmailMessageAggs {
  subject: AggOtherScalar
  template: AggOtherScalar
  text: AggOtherScalar
  html: AggOtherScalar
  deliveryTime: AggOtherScalar
  requireTls: AggOtherScalar
  skipVerification: AggOtherScalar
  headers: AggOtherScalar
  variables: AggOtherScalar
  recipientVariables: AggOtherScalar
  tags: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  from_value: AggOtherScalar
  from_type: AggOtherScalar
  to_value: AggOtherScalar
  to_type: AggOtherScalar
  cc_value: AggOtherScalar
  cc_type: AggOtherScalar
  bcc_value: AggOtherScalar
  bcc_type: AggOtherScalar
}

"""Scalar fields defined by HypiEnv"""
enum HypiEnvScalarFields {
  apiHost
  websocketHost
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type GroupPolicyAggs {
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by TimePolicy"""
enum TimePolicyScalarFields {
  """
  (yyyy-MM-dd hh:mm:ss) can be used for example to ensure a file is not viewable before the given date
  """
  from

  """can be used to ensure a file is not viewable after a given date"""
  to
  name

  """Positive` or `Negative"""
  logic
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type RequestTemplateAggs {
  name: AggOtherScalar
  request: AggOtherScalar
  response: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by AggOtherScalar"""
enum AggOtherScalarScalarFields {
  count
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

type WorkflowAggs {
  name: AggOtherScalar
  cronSchedule: AggOtherScalar
  execAs: AggOtherScalar
  async: AggOtherScalar
  parallel: AggOtherScalar
  maxExecutionTime: AggOtherScalar
  repeatN: AggInt
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  evaluateIf_type: AggOtherScalar
  evaluateIf_field: AggOtherScalar
  evaluateIf_selection: AggOtherScalar
  repeatIf_type: AggOtherScalar
  repeatIf_field: AggOtherScalar
  repeatIf_selection: AggOtherScalar
}

input ScriptInput {
  hypi: HypiInput
  type: TanType!
  name: String!
  body: String!
}

input MessageGroupGroupByOptions {
  """The field by which to to group the matching data"""
  field: MessageGroupScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input CounterGroupByOptions {
  """The field by which to to group the matching data"""
  field: CounterScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

""" namespace for containing authz objects and their relationships."""
type Realm {
  hypi: Hypi

  """
  the name identifying the organisation and becomes the URL by which it is accessed e.g. alpha-corp.hypi.app, where alpha-corp is name
  If not provided one will be automatically generated
  """
  name: String
  logo: Image

  """The name displayed in the user interface"""
  displayName: String

  """If true users can register without an admin creating their account"""
  allowRegistrations: Boolean

  """if true users must verify their email before they're allowed to login"""
  verifyEmail: Boolean

  """
  Optionally defines some options to help detect and protect against brute force login attempts
  """
  bruteForceDetection: BruteForceDetectionOptions
  referrer: String
  remoteLoginId: String
  organisations(arcql: String, first: Int, after: String, last: Int, before: String): [Organisation!]!
}

"""Scalar fields defined by WorkflowSession"""
enum WorkflowSessionScalarFields {
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input PolicyInput {
  realms: [RealmLinkInput!]
  clients: [AuthClientInput!]
  roles: [RoleInput!]
  name: String!
  policies: [PolicyInput!]
  groups: [GroupInput!]
  from: DateTime
  hypi: HypiInput
  logic: AuthLogic
  to: DateTime
  accounts: [AccountInput!]
  decisionStrategy: DecisionStrategy
}

type Password {
  hypi: Hypi

  """
   password is never returned
   further, the @secret directive enforces this, queries can be use to perform comparison against the field but it is never returned
   
  """
  value: String!
  expired: Boolean
}

type Pair {
  hypi: Hypi
  key: String
  value: String
}

input OAuth2AuthorizedClientInputOpt {
  hypi: HypiInputOpt
  clientRegistrationId: String
  principalName: String
  accessToken: String
  refreshToken: String
}

input LanguageInput {
  hypi: HypiInput
  family: String
  isoName: String
  nativeName: String
  iso6391Code: String
  iso6392TCode: String
  iso6392BCode: String
  iso6393Code: String
}

"""All fields defined by RealmPolicy"""
enum RealmPolicyFields {
  hypi
  name

  """Positive` or `Negative"""
  logic
  realms
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input GeoEnvelopeInputOpt {
  hypi: HypiInputOpt
  p1: CoordinateInputOpt
  p2: CoordinateInputOpt
}

"""Scalar fields defined by Organisation"""
enum OrganisationScalarFields {
  name
  incorporated
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""A list of all types in the app"""
enum HypiSchemaType {
  Message
  User
  MessageGroup
  UserStory
  PageInfo
  HypiResultEdge
  HypiFilterConnection
  HypiEnv
  Pair
  AggInt
  AggFloat
  AggOtherScalar
  Script

  """Defines the templates that should be applied to a given HTTP request"""
  RequestTemplate
  NotificationCtx
  Notification
  URL
  Currency
  Coordinate
  GeoEnvelope
  Language
  Address
  PersonName
  Phone
  Email
  Password
  RemoteLogin
  LoginAttempt
  BruteForceDetectionOptions
  OAuth2AuthorizedClient

  """
  A client defines an agent that acts on behalf of a user/subject.
  Currently implicitly created by Hypi.
  """
  AuthClient
  ABACPolicy
  ABACTag
  Image
  EmailVerification

  """
  Defines a Hypi template that can be parameterised
  https://documentation.Hypi.com/en/latest/api-templates.html#store-new-template
  """
  EmailTemplate
  EmailSendingAttempt

  """
  To reset an Account's password, create a `PasswordReminder`.
  
  This will generate a code in the `code` field that can be referenced using $!{parent.code} in the `htmlMessage` or `plainTextMessage` fields.
  
  This will send an email to the email in the `to` field. In the message you should provide a link to a URL where the user can enter their new password.
  Include the code in this URL e.g. https://my-app.com/reset-password?code=$!{parent.code}.
  
  When the user gets to this page, you will have the password reset code in the URL query string. Get this code from the URL
  and when the user enter their new password, make a POST request to the Hypi API e.g.
  POST <hypi-domain>/email/reset/<domain> - where <domain> is app instance domain.
  
  In the body of the request send a JSON like this:
  {"code": "<the-code-from-the-URL>", "password": "<the-user's-new-password>"}
  
  Hypi will change the user's password and return HTTP status 200.
  """
  PasswordReminder

  """
  Defines a web hook that can be used to trigger Hypi GraphQL functions on a given app.
  The account specified in the web hook must have access to the app/instance and must be authorised to call the functions specified.
  """
  Webhook

  """
  If the query or mutation functions in the Webhook definition returns this then it controls what the server responds with
  For example, the GraphQL function can return a 301 or 302 status and a Location header to an external URL to cause a redirect.
  """
  WebhookResponse
  LogMessage

  """Creates a reference to a GraphQL function in an app instance."""
  GraphQLRef
  WorkflowStepData
  WorkflowStep
  AccessToken

  """
  A row is created for every resource. It cannot be created or modified by end users.
  The ID of each entry is a hash of the resource ID, type and field.
  When the resource is deleted, the entry is deleted.
  """
  StorageCounter
  PermissionDescription
  Hypi

  """
  Identifies a given country according to ISO3166
  https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
  See also https://www.iso.org/obp/ui/#search and
  https://unicode-org.github.io/cldr-staging/charts/37/supplemental/territory_information.html
  """
  Country
  Account
  Person
  Organisation
  OAuthProvider

  """ namespace for containing authz objects and their relationships."""
  Realm

  """Defines a collection for subjects, roles, policies and permissions."""
  Group
  Role
  RolePolicy
  ClientPolicy
  TimePolicy
  AggregatedPolicy
  GroupPolicy
  AccountPolicy
  RealmPolicy
  RealmLink
  Permission
  File
  Video

  """
  Creates a new outbound message.
  Note that it automatically send unless the autoSend field is false
  """
  EmailMessage

  """
  A workflow defines a sequence of steps that execute in a defined order (you set the order field on each step).
  It is one way in which Hypi allows you to do composition, so similarities can be drawn to function composition with some specifics thrown in for Hypi and GraphQL.
  
  If two steps have the same order their execution order is undefined with relation to each other.
  Every step has a GraphQL function that is executed for that step.
  
  When a Workflow is executed it creates a WorkflowSession. The result of each step in the workflow is added to the session.
  
  The first step can have any parameters you want. For the other steps there are some rules that define how the system maps
  parameters to the function in these steps. These rules are:
  
  1. Any step (including the first step) can have a parameter "params: Json" i.e. name = params and type is Json.
  This params is a map of the arguments passed to the first function in the Workflow. For example if the function was defined as
  step1(a: Int, b: Json, c: MyType): T
  in this case the "params" Json object would be have the fields a, b and c set to the values the function was executed with.
  Normally, this is used in the first step but can be used in any step that wants access to this data.
  
  2. Any step can have a parameter "session: WorkflowSession" - this is the current workflow's session and contains the results of all steps before the current one.
  You can identify the results for a specific step by finding the result using the step's name in the session's data array.
  
  3. Except the first step, a parameter "previous: T" where T is the result type of the previous step can be used.
  In this case, the platform will use the output of the previous function for this parameter.
  Note that if the type is not the same as the last step's output type then the workflow will fail if the field is not optional.
  If the field is optional then the platform will not provide it and it would therefore be null if you try to use it.
  
  4. Except the first step, pass-through is possible. This is where the parameters from the first step are passed through
  to other steps by name and type. i.e. given
  step1(a: Int, b: String): String
  step2(a: Int): ID
  In this case, the variable "a" in both step1 and step2 will have the same value that step1 was executed with.
  Incidentally, this is the same as getting "a" from the "params" Json.
  """
  Workflow
  WorkflowSession
  Counter
  Gauge
  ServerlessResponse
}

"""All fields defined by AuthClient"""
enum AuthClientFields {
  hypi
  name
  secret
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type RealmLinkAggs {
  name: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by ABACPolicy"""
enum ABACPolicyScalarFields {
  from
  to

  """
   The instance the policy applies to. By default the same instance in which it exists.
   A `Platform Admin` can set it to *, all other users get permission denied if this is not the same as their current instance.
   
  """
  givenInstance

  """e.g. Account or *"""
  givenType

  """Exactly one of Query|Mutation|Subscription|*"""
  givenOperation

  """The exact function name or wildcard e.g. find|upsert|*"""
  givenFn

  """
  The prefix that any function can begin with e.g. find will match findX, findY, findOther
  """
  givenFnPrefix
  whenResourceTagKeyEq
  whenResourceTagKeyPrefix
  whenResourceTagValueEq
  whenResourceTagValuePrefix

  """Policy applies when the account ID is equal to this"""
  assertAccountIdEq

  """Policy applies when the account username starts with this"""
  assertAccountUsernamePrefix

  """When set, the account MUST have a tag whose key is equal to this"""
  assertAccountTagKeyEq

  """When set, the account MUST have a tag whose key is starts with this"""
  assertAccountTagKeyPrefix

  """When set, the account MUST have a tag whose value is equal to this"""
  assertAccountTagValEq

  """When set, the account MUST have a tag whose value is starts with this"""
  assertAccountTagValPrefix

  """
  Resource owner can set the boundary to RESOURCE (or anyone that has permission to do so)
  System Admin for the instance can set the boundary to INSTANCE
  Platform Admin can set the boundary to PLATFORM
  PLATFORM|INSTANCE|RESOURCE
  """
  boundary

  """
  If provided, this is a comma separate list of field paths that are allowed by this policy
  e.g. a,b.c allows access to a and all sub-fields below it as well as to the field c under the parent field b. No other field under b is allowed
  If the policy is allowing read access, only these fields can be seen. If it is write acces, only these fields can be modified.
  """
  allowedFields
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

type NotificationAggs {
  message: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  ctx_type: AggOtherScalar
  ctx_targetAccount: AggOtherScalar
}

type BruteForceDetectionOptionsAggs {
  maxLoginFailures: AggInt
  waitIncrements: AggInt
  waitIncrementsUnit: AggOtherScalar
  quickLoginCheckMillis: AggInt
  minQuickLoginWait: AggInt
  minQuickLoginWaitUnit: AggOtherScalar
  maxWait: AggInt
  maxWaitUnit: AggOtherScalar
  failureReset: AggInt
  failureResetUnit: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type RemoteLoginAggs {
  type: AggOtherScalar
  email: AggOtherScalar
  remoteId: AggOtherScalar
  otherAttributes: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input RolePolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: RolePolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type AccountPolicyAggs {
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by Password"""
enum PasswordScalarFields {
  """
   password is never returned
   further, the @secret directive enforces this, queries can be use to perform comparison against the field but it is never returned
   
  """
  value
  expired
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""Scalar fields defined by Account"""
enum AccountScalarFields {
  verified
  enabled
  username
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input OAuthProviderInputOpt {
  hypi: HypiInputOpt
  clientId: String
  clientSecret: String
  clientAuthenticationMethod: ClientAuthenticationMethod
  authorizationGrantType: AuthorizationGrantType
  redirectUriTemplate: String
  scopes: [String]
  authorizationUri: String
  tokenUri: String
  userInfoUri: String
  userInfoAuthenticationMethod: AuthenticationMethod
  userNameAttributeName: UserNameAttributeName
  jwkSetUri: String
  configurationMetadata: [PairInputOpt]
  clientName: String
  hypiSuccessRedirectUri: String
  hypiFailureRedirectUri: String
}

type Account {
  hypi: Hypi
  verified: Boolean
  enabled: Boolean
  username: String!
  password: Password!
  owner: Person
  emails(arcql: String, first: Int, after: String, last: Int, before: String): [Email!]
  phones(arcql: String, first: Int, after: String, last: Int, before: String): [Phone!]
  groups(arcql: String, first: Int, after: String, last: Int, before: String): [Group!]
  roles(arcql: String, first: Int, after: String, last: Int, before: String): [Role!]
  attempts(arcql: String, first: Int, after: String, last: Int, before: String): [LoginAttempt!]
  remoteLogins(arcql: String, first: Int, after: String, last: Int, before: String): [RemoteLogin!]
}

enum AuthLogic {
  """Access will be granted"""
  Positive

  """Access will be denied"""
  Negative
}

"""Scalar fields defined by Language"""
enum LanguageScalarFields {
  family
  isoName
  nativeName
  iso6391Code
  iso6392TCode
  iso6392BCode
  iso6393Code
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""Scalar fields defined by UserStory"""
enum UserStoryScalarFields {
  text
  img
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input WorkflowRepeatableInput {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInput
  repeatN: Int
  steps: [WorkflowStepInput!]
  cronSchedule: String
  repeatIf: GraphQLRefInput
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int
}

input WebhookResponseGroupByOptions {
  """The field by which to to group the matching data"""
  field: WebhookResponseScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input PermissionInputOpt {
  hypi: HypiInputOpt
  name: String
  policies: [PolicyInputOpt]
  decisionStrategy: DecisionStrategy
  type: String
  scopes: [String]
  resource: String
  operationType: OpType
  operations: [String]
  includeAllAccounts: Boolean
}

input PasswordReminderInputOpt {
  hypi: HypiInputOpt
  valid: Boolean
  code: String
  to: EmailInputOpt
  from: String
  subject: String
  htmlMessage: String
  plainTextMessage: String
}

input UserGroupByOptions {
  """The field by which to to group the matching data"""
  field: UserScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input WorkflowSessionInput {
  hypi: HypiInput
  data: [WorkflowStepDataInput!]
}

"""Scalar fields defined by WorkflowStep"""
enum WorkflowStepScalarFields {
  """A name that can be used to reference or trigger this step"""
  name
  order

  """
  An ArcQL query to find the account e.g. hypi.id = 'user123' to find by id or username = 'blah' to find by username
  If present, execution of the steps in the Workflow will be done as this account
  If not specified, it defaults to the account making the request
  """
  execAs
  async

  """
   Specifies the the max time an async task should be allowed to execute. When this time has elapsed the task will be killed.
   The format is ISO8601 durations https://en.wikipedia.org/wiki/ISO_8601#Durations
   e.g. P1M is 1 month and PT1M is 1 minute
   
  """
  maxExecutionTime
  repeatN
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_type

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_field

  """
  The function to execute for this step, the data returned by the step can subsequently be used in other steps
  """
  fn_selection
  evaluateIf_type
  evaluateIf_field
  evaluateIf_selection
  repeatIf_type
  repeatIf_field
  repeatIf_selection
}

input RolePolicyInputOpt {
  hypi: HypiInputOpt
  roles: [RoleInputOpt]
  name: String
  logic: AuthLogic
}

type EmailTemplateAggs {
  name: AggOtherScalar
  description: AggOtherScalar
  template: AggOtherScalar
  comment: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input GaugeInput {
  hypi: HypiInput
  name: String!
  label: String
  tags: [String!]
  value: Float!
}

"""All fields defined by EmailVerification"""
enum EmailVerificationFields {
  hypi
  email

  """
  After the link is clicked from the email, redirect the browser to this URL passing a token in the URL i.e. token=jwt.token.here which can be used get the value in the meta field
  """
  redirectTo

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject
  templateName

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage

  """
  Any additional meta data you want to store. For example, you could collect all of the information needed to create the Account
  """
  meta

  """Set by system, cannot be provided"""
  confirmed
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
  email_hypi
  email_value
  email_type
}

type Subscription {
  """Subscribe to mutations on the given type."""
  subscribe(id: String): HypiSubscriptionUnion!
}

"""All fields defined by Address"""
enum AddressFields {
  hypi
  door
  street
  town
  county
  city
  country
  postCode

  """  country: Country"""
  from
  to
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by Organisation"""
enum OrganisationFields {
  hypi
  name
  logo
  incorporated
  addresses
  phones
  emails
  members
  subsidiaries
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input WebhookResponseInputOpt {
  hypi: HypiInputOpt
  status: Int
  headers: Json
  body: Json
}

"""All fields defined by AggregatedPolicy"""
enum AggregatedPolicyFields {
  hypi

  """defines how the policy arrives at a decision, the options are:"""
  decisionStrategy
  name

  """Positive` or `Negative"""
  logic
  policies
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""All fields defined by ABACTag"""
enum ABACTagFields {
  hypi
  key

  """If provided then policy assertion can be made against it"""
  value
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""
The precedence of the operations follows BODMAS. https://en.wikipedia.org/wiki/Order_of_operations
For clarity if all fields are specified the precedence is:
1. Divsion
2. Multiplication
3. Subtraction
4. Addition
"""
input MathInputInt {
  div: Int
  times: Int
  minus: Int
  plus: Int
  hypi: HypiInput!
}

input PasswordInput {
  hypi: HypiInput
  value: String!
  expired: Boolean
}

"""Scalar fields defined by RequestTemplate"""
enum RequestTemplateScalarFields {
  name
  request
  response
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input GaugeInputOpt {
  hypi: HypiInputOpt
  name: String
  label: String
  tags: [String]
  value: Float
}

type BruteForceDetectionOptions {
  hypi: Hypi
  maxLoginFailures: Int!

  """How long the user ust wait when maxLoginFailures have been reached"""
  waitIncrements: Int
  waitIncrementsUnit: TimeUnit

  """
  If login failures occurr too quickly, lock out the user, this sets number of milliseconds that determine "quickly"
  """
  quickLoginCheckMillis: Int

  """How long to wait after a quick failure lock out"""
  minQuickLoginWait: Int
  minQuickLoginWaitUnit: TimeUnit

  """max time a user will be locked out for"""
  maxWait: Int
  maxWaitUnit: TimeUnit

  """When failure count is reset"""
  failureReset: Int
  failureResetUnit: TimeUnit
}

"""Scalar fields defined by Email"""
enum EmailScalarFields {
  value
  type
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input GroupGroupByOptions {
  """The field by which to to group the matching data"""
  field: GroupScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by Video"""
enum VideoScalarFields {
  name
  description
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input GeoEnvelopeInput {
  hypi: HypiInput
  p1: CoordinateInput!
  p2: CoordinateInput!
}

"""
Based on http://www.tsusiatsoftware.net/
See Geometry Model at http://www.tsusiatsoftware.net/jts/jtsfeatures.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/Geometry.html
Defines a rectangular region of the 2D coordinate plane. It is often used to represent the bounding box of a Geometry, e.g. the minimum and maximum x and y values of the Coordinates.
todo define specific subtypes of Geo and supporting types
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/PrecisionModel.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/Point.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/MultiPoint.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/LineString.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/MultiLineString.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/Polygon.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/MultiPolygon.html
http://www.tsusiatsoftware.net/jts/javadoc/com/vividsolutions/jts/geom/GeometryCollection.html
"""
interface Geo {
  hypi: Hypi
  envelope: GeoEnvelope
  srid: Int
}

type WebhookResponseAggs {
  status: AggInt
  headers: AggOtherScalar
  body: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type Coordinate {
  hypi: Hypi
  x: Float!
  y: Float!
}

type GroupPolicy implements Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  groups(arcql: String, first: Int, after: String, last: Int, before: String): [Group!]
}

"""
Identifies a given country according to ISO3166
https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
See also https://www.iso.org/obp/ui/#search and
https://unicode-org.github.io/cldr-staging/charts/37/supplemental/territory_information.html
"""
type Country {
  hypi: Hypi
  name: String!
  stateName: String
  sovereignty: String
  alpha2code: String
  alpha3code: String
  numericCode: String
  subdivisionCodeLinks: String
  internetCCTLD: String
  continent: String
  officialLanguage: Language
  currencies(arcql: String, first: Int, after: String, last: Int, before: String): [Currency!]
  languagesSpoken(arcql: String, first: Int, after: String, last: Int, before: String): [Language!]
}

"""Scalar fields defined by Image"""
enum ImageScalarFields {
  name
  description
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input UserStoryInput {
  hypi: HypiInput
  text: String
  img: String
}

input FileGroupByOptions {
  """The field by which to to group the matching data"""
  field: FileScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type CounterAggs {
  name: AggOtherScalar
  label: AggOtherScalar
  value: AggFloat
  tags: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input WorkflowSessionGroupByOptions {
  """The field by which to to group the matching data"""
  field: WorkflowSessionScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by PasswordReminder"""
enum PasswordReminderScalarFields {
  """If true the reset code has not yet been used."""
  valid

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """The Account email that needs to be changed"""
  to_value

  """The Account email that needs to be changed"""
  to_type
}

input PhoneInputOpt {
  hypi: HypiInputOpt
  number: String
  country: CountryInputOpt
  code: String
}

"""Scalar fields defined by Phone"""
enum PhoneScalarFields {
  number
  code
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""A list of all types in the app which can created or updated directly"""
enum HypiMutationType {
  Message
  User
  MessageGroup
  UserStory
  PageInfo
  HypiResultEdge
  HypiFilterConnection
  HypiEnv
  Pair
  AggInt
  AggFloat
  AggOtherScalar
  Script

  """Defines the templates that should be applied to a given HTTP request"""
  RequestTemplate
  NotificationCtx
  Notification
  URL
  Currency
  Coordinate
  GeoEnvelope
  Language
  Address
  PersonName
  Phone
  Email
  Password
  RemoteLogin
  LoginAttempt
  BruteForceDetectionOptions
  OAuth2AuthorizedClient

  """
  A client defines an agent that acts on behalf of a user/subject.
  Currently implicitly created by Hypi.
  """
  AuthClient
  ABACPolicy
  ABACTag
  Image
  EmailVerification

  """
  Defines a Hypi template that can be parameterised
  https://documentation.Hypi.com/en/latest/api-templates.html#store-new-template
  """
  EmailTemplate
  EmailSendingAttempt

  """
  To reset an Account's password, create a `PasswordReminder`.
  
  This will generate a code in the `code` field that can be referenced using $!{parent.code} in the `htmlMessage` or `plainTextMessage` fields.
  
  This will send an email to the email in the `to` field. In the message you should provide a link to a URL where the user can enter their new password.
  Include the code in this URL e.g. https://my-app.com/reset-password?code=$!{parent.code}.
  
  When the user gets to this page, you will have the password reset code in the URL query string. Get this code from the URL
  and when the user enter their new password, make a POST request to the Hypi API e.g.
  POST <hypi-domain>/email/reset/<domain> - where <domain> is app instance domain.
  
  In the body of the request send a JSON like this:
  {"code": "<the-code-from-the-URL>", "password": "<the-user's-new-password>"}
  
  Hypi will change the user's password and return HTTP status 200.
  """
  PasswordReminder

  """
  Defines a web hook that can be used to trigger Hypi GraphQL functions on a given app.
  The account specified in the web hook must have access to the app/instance and must be authorised to call the functions specified.
  """
  Webhook

  """
  If the query or mutation functions in the Webhook definition returns this then it controls what the server responds with
  For example, the GraphQL function can return a 301 or 302 status and a Location header to an external URL to cause a redirect.
  """
  WebhookResponse
  LogMessage

  """Creates a reference to a GraphQL function in an app instance."""
  GraphQLRef
  WorkflowStepData
  WorkflowStep
  AccessToken

  """
  A row is created for every resource. It cannot be created or modified by end users.
  The ID of each entry is a hash of the resource ID, type and field.
  When the resource is deleted, the entry is deleted.
  """
  StorageCounter
  PermissionDescription
  Hypi

  """
  Identifies a given country according to ISO3166
  https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
  See also https://www.iso.org/obp/ui/#search and
  https://unicode-org.github.io/cldr-staging/charts/37/supplemental/territory_information.html
  """
  Country
  Account
  Person
  Organisation
  OAuthProvider

  """ namespace for containing authz objects and their relationships."""
  Realm

  """Defines a collection for subjects, roles, policies and permissions."""
  Group
  Role
  RolePolicy
  ClientPolicy
  TimePolicy
  AggregatedPolicy
  GroupPolicy
  AccountPolicy
  RealmPolicy
  RealmLink
  Permission
  File
  Video

  """
  Creates a new outbound message.
  Note that it automatically send unless the autoSend field is false
  """
  EmailMessage

  """
  A workflow defines a sequence of steps that execute in a defined order (you set the order field on each step).
  It is one way in which Hypi allows you to do composition, so similarities can be drawn to function composition with some specifics thrown in for Hypi and GraphQL.
  
  If two steps have the same order their execution order is undefined with relation to each other.
  Every step has a GraphQL function that is executed for that step.
  
  When a Workflow is executed it creates a WorkflowSession. The result of each step in the workflow is added to the session.
  
  The first step can have any parameters you want. For the other steps there are some rules that define how the system maps
  parameters to the function in these steps. These rules are:
  
  1. Any step (including the first step) can have a parameter "params: Json" i.e. name = params and type is Json.
  This params is a map of the arguments passed to the first function in the Workflow. For example if the function was defined as
  step1(a: Int, b: Json, c: MyType): T
  in this case the "params" Json object would be have the fields a, b and c set to the values the function was executed with.
  Normally, this is used in the first step but can be used in any step that wants access to this data.
  
  2. Any step can have a parameter "session: WorkflowSession" - this is the current workflow's session and contains the results of all steps before the current one.
  You can identify the results for a specific step by finding the result using the step's name in the session's data array.
  
  3. Except the first step, a parameter "previous: T" where T is the result type of the previous step can be used.
  In this case, the platform will use the output of the previous function for this parameter.
  Note that if the type is not the same as the last step's output type then the workflow will fail if the field is not optional.
  If the field is optional then the platform will not provide it and it would therefore be null if you try to use it.
  
  4. Except the first step, pass-through is possible. This is where the parameters from the first step are passed through
  to other steps by name and type. i.e. given
  step1(a: Int, b: String): String
  step2(a: Int): ID
  In this case, the variable "a" in both step1 and step2 will have the same value that step1 was executed with.
  Incidentally, this is the same as getting "a" from the "params" Json.
  """
  Workflow
  WorkflowSession
  Counter
  Gauge
  ServerlessResponse
}

input RemoteLoginInputOpt {
  hypi: HypiInputOpt
  type: String
  email: String
  remoteId: String
  otherAttributes: Json
}

input WorkflowConditionalInput {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInput
  repeatN: Int
  steps: [WorkflowStepInput!]
  cronSchedule: String
  repeatIf: GraphQLRefInput
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int
}

"""All fields defined by PermissionDescription"""
enum PermissionDescriptionFields {
  hypi
  roles
  groups
  organisations
  realms
  permissions
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type MessageAggs {
  text: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by Hypi"""
enum HypiScalarFields {
  """
  An ID automatically generated by the platform for new objects.
  If provided and the ID does not exist, the provided ID is used instead of a generated one and a new entry is inserted
  If provided and the ID already exists then the existing object is updated.
  """
  id

  """
  When you work with interface fields, Hypi is unable to distinguish which implementation you intend to use
  automatically, you must set this field to the name of the implementation of the interface e.g.
  If creating an AccountPolicy which implements the Policy interface, this field should be set to AccountPolicy
  """
  impl

  """The ISO8601 date of when the object was created"""
  created

  """The ISO8601 date of when the object was last modified"""
  updated

  """
  The ISO8601 date of when the object was trashed (if it is currently trashed, null otherwise)
  """
  trashed

  """The ID of the account which created the object"""
  createdBy

  """The ID of the app instance which created and owns the object"""
  instanceId
}

input LoginAttemptGroupByOptions {
  """The field by which to to group the matching data"""
  field: LoginAttemptScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type CoordinateAggs {
  x: AggFloat
  y: AggFloat
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input URLInputOpt {
  hypi: HypiInputOpt
  path: String
  queryParams: Json
  port: Int
  host: String
}

"""A list of types which can be subscribed to"""
type HypiSubscriptionUnion {
  Message: Message
  User: User
  MessageGroup: MessageGroup
  UserStory: UserStory
  PageInfo: PageInfo
  HypiResultEdge: HypiResultEdge
  HypiFilterConnection: HypiFilterConnection
  HypiEnv: HypiEnv
  Pair: Pair
  AggInt: AggInt
  AggFloat: AggFloat
  AggOtherScalar: AggOtherScalar
  Script: Script
  RequestTemplate: RequestTemplate
  NotificationCtx: NotificationCtx
  Notification: Notification
  URL: URL
  Currency: Currency
  Coordinate: Coordinate
  GeoEnvelope: GeoEnvelope
  Language: Language
  Address: Address
  PersonName: PersonName
  Phone: Phone
  Email: Email
  Password: Password
  RemoteLogin: RemoteLogin
  LoginAttempt: LoginAttempt
  BruteForceDetectionOptions: BruteForceDetectionOptions
  OAuth2AuthorizedClient: OAuth2AuthorizedClient
  AuthClient: AuthClient
  ABACPolicy: ABACPolicy
  ABACTag: ABACTag
  Image: Image
  EmailVerification: EmailVerification
  EmailTemplate: EmailTemplate
  EmailSendingAttempt: EmailSendingAttempt
  PasswordReminder: PasswordReminder
  Webhook: Webhook
  WebhookResponse: WebhookResponse
  LogMessage: LogMessage
  GraphQLRef: GraphQLRef
  WorkflowStepData: WorkflowStepData
  WorkflowStep: WorkflowStep
  AccessToken: AccessToken
  StorageCounter: StorageCounter
  PermissionDescription: PermissionDescription
  Hypi: Hypi
  Country: Country
  Account: Account
  Person: Person
  Organisation: Organisation
  OAuthProvider: OAuthProvider
  Realm: Realm
  Group: Group
  Role: Role
  RolePolicy: RolePolicy
  ClientPolicy: ClientPolicy
  TimePolicy: TimePolicy
  AggregatedPolicy: AggregatedPolicy
  GroupPolicy: GroupPolicy
  AccountPolicy: AccountPolicy
  RealmPolicy: RealmPolicy
  RealmLink: RealmLink
  Permission: Permission
  File: File
  Video: Video
  EmailMessage: EmailMessage
  Workflow: Workflow
  WorkflowSession: WorkflowSession
  Counter: Counter
  Gauge: Gauge
  ServerlessResponse: ServerlessResponse
}

input ABACTagGroupByOptions {
  """The field by which to to group the matching data"""
  field: ABACTagScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input ServerlessResponseGroupByOptions {
  """The field by which to to group the matching data"""
  field: ServerlessResponseScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""A list of types on which mathematical mutations can be performed on"""
input HypiMathType {
  URL: [URLMaths!]
  Coordinate: [CoordinateMaths!]
  BruteForceDetectionOptions: [BruteForceDetectionOptionsMaths!]
  WebhookResponse: [WebhookResponseMaths!]
  WorkflowStep: [WorkflowStepMaths!]
  StorageCounter: [StorageCounterMaths!]
  Workflow: [WorkflowMaths!]
  Counter: [CounterMaths!]
  Gauge: [GaugeMaths!]
}

type PasswordAggs {
  value: AggOtherScalar
  expired: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type WorkflowStepAggs {
  name: AggOtherScalar
  order: AggInt
  execAs: AggOtherScalar
  async: AggOtherScalar
  maxExecutionTime: AggOtherScalar
  repeatN: AggInt
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  fn_type: AggOtherScalar
  fn_field: AggOtherScalar
  fn_selection: AggOtherScalar
  evaluateIf_type: AggOtherScalar
  evaluateIf_field: AggOtherScalar
  evaluateIf_selection: AggOtherScalar
  repeatIf_type: AggOtherScalar
  repeatIf_field: AggOtherScalar
  repeatIf_selection: AggOtherScalar
}

"""All fields defined by OAuth2AuthorizedClient"""
enum OAuth2AuthorizedClientFields {
  hypi
  clientRegistrationId
  principalName
  accessToken
  refreshToken
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type OAuthProviderAggs {
  clientId: AggOtherScalar
  clientSecret: AggOtherScalar
  clientAuthenticationMethod: AggOtherScalar
  authorizationGrantType: AggOtherScalar
  redirectUriTemplate: AggOtherScalar
  authorizationUri: AggOtherScalar
  tokenUri: AggOtherScalar
  userInfoUri: AggOtherScalar
  userInfoAuthenticationMethod: AggOtherScalar
  userNameAttributeName: AggOtherScalar
  jwkSetUri: AggOtherScalar
  clientName: AggOtherScalar
  hypiSuccessRedirectUri: AggOtherScalar
  hypiFailureRedirectUri: AggOtherScalar
  scopes: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

"""Scalar fields defined by Role"""
enum RoleScalarFields {
  name
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input WorkflowMaths {
  repeatN: MathInputInt
}

type LogMessage {
  hypi: Hypi
  level: LogLevel!
  message: String
  stackTrace: String

  """Optional, may not be a stacktrace"""
  releaseId: String

  """This is optional, we can have system messages that aren't from an app"""
  type: String

  """
  The name of the GraphQL type that the log is for, this is also optional
  """
  workflow: String
}

input AddressGroupByOptions {
  """The field by which to to group the matching data"""
  field: AddressScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type CountryAggs {
  name: AggOtherScalar
  stateName: AggOtherScalar
  sovereignty: AggOtherScalar
  alpha2code: AggOtherScalar
  alpha3code: AggOtherScalar
  numericCode: AggOtherScalar
  subdivisionCodeLinks: AggOtherScalar
  internetCCTLD: AggOtherScalar
  continent: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input WebhookInput {
  hypi: HypiInput
  name: String
  as: AccountInput
  query: GraphQLRefInput!
}

input EmailVerificationGroupByOptions {
  """The field by which to to group the matching data"""
  field: EmailVerificationScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""Scalar fields defined by AggInt"""
enum AggIntScalarFields {
  avg
  count
  max
  min
  sum
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

"""All fields defined by Language"""
enum LanguageFields {
  hypi
  family
  isoName
  nativeName
  iso6391Code
  iso6392TCode
  iso6392BCode
  iso6393Code
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

interface Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
}

type ScriptAggs {
  type: AggOtherScalar
  name: AggOtherScalar
  body: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

input UserInputOpt {
  hypi: HypiInputOpt
  account: AccountInputOpt
  stories: [UserStoryInputOpt]
}

type AggInt {
  hypi: Hypi

  """The value of the aggregated field for each group"""
  groupValues: [Pair]
  avg(distinct: Boolean): Float
  count(distinct: Boolean): Int
  max: Int
  min: Int
  sum(distinct: Boolean): Int
}

input AggregatedPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: AggregatedPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input WorkflowStepMaths {
  order: MathInputInt
  repeatN: MathInputInt
}

input WorkflowTimedInput {
  execAs: String
  maxExecutionTime: String
  fn: GraphQLRefInput
  repeatN: Int
  steps: [WorkflowStepInput!]
  cronSchedule: String
  repeatIf: GraphQLRefInput
  async: Boolean
  parallel: Boolean
  name: String
  hypi: HypiInput
  evaluateIf: GraphQLRefInput
  order: Int
}

input TimePolicyInputOpt {
  hypi: HypiInputOpt
  from: DateTime
  to: DateTime
  clients: [AuthClientInputOpt]
  roles: [RoleInputOpt]
  groups: [GroupInputOpt]
  accounts: [AccountInputOpt]
  realms: [RealmLinkInputOpt]
  name: String
  logic: AuthLogic
}

"""All fields defined by Script"""
enum ScriptFields {
  hypi
  type
  name
  body
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type AccessToken {
  hypi: Hypi
  sessionToken: String
  sessionExpires: Long
  errorCode: String
  errorMsg: String
}

type PersonAggs {
  dob: AggOtherScalar
  gender: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

type Script {
  hypi: Hypi
  type: TanType!
  name: String!
  body: String!
}

"""
Creates a new outbound message.
Note that it automatically send unless the autoSend field is false
"""
type EmailMessage {
  hypi: Hypi
  from: Email!
  subject: String!

  """
  Name of the template to use, if present then the given template is used and text/html etc fields in this message are not used
  """
  template: String
  text: String
  html: String

  """
   Schedule sending in the future
   
  """
  deliveryTime: DateTime

  """
   If set to True or yes this requires the message only be sent over a TLS connection. If a TLS connection can not be established, we will not deliver the message.
   If set to False or no, we will still try and upgrade the connection, if that fails the message will be delivered over a plaintext SMTP connection.
   
  """
  requireTls: Boolean

  """
   If set to True or yes, the certificate and hostname will not be verified when trying to establish a TLS connection and Hypi will accept any certificate during delivery.
  
   If set to False or no, Hypi will verify the certificate and hostname. If either one can not be verified, a TLS connection will not be established.
  
   The default is False.
   
  """
  skipVerification: Boolean

  """
   allows to append a custom MIME header to the message (X-My-Header in this case). For example, h:Reply-To to specify Reply-To address.
   
  """
  headers: Json

  """
   prefix followed by an arbitrary name allows to attach a custom JSON data to the message. See Attaching Data to Messages for more information.
   
  """
  variables: Json

  """
   A valid JSON-encoded dictionary, where key is a plain recipient address and value is a dictionary with variables that can be referenced in the message body.
   
  """
  recipientVariables: Json
  to(arcql: String, first: Int, after: String, last: Int, before: String): [Email!]!
  cc(arcql: String, first: Int, after: String, last: Int, before: String): [Email!]
  bcc(arcql: String, first: Int, after: String, last: Int, before: String): [Email!]
  attachment(arcql: String, first: Int, after: String, last: Int, before: String): [File!]
  inline(arcql: String, first: Int, after: String, last: Int, before: String): [File!]
  tags(arcql: String, first: Int, after: String, last: Int, before: String): [String]
  responses(arcql: String, first: Int, after: String, last: Int, before: String): [EmailSendingAttempt!]
}

"""All fields defined by AccountPolicy"""
enum AccountPolicyFields {
  hypi
  name

  """Positive` or `Negative"""
  logic
  accounts
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input RealmInputOpt {
  hypi: HypiInputOpt
  name: String
  logo: ImageInputOpt
  displayName: String
  allowRegistrations: Boolean
  verifyEmail: Boolean
  bruteForceDetection: BruteForceDetectionOptionsInputOpt
  organisations: [OrganisationInputOpt]
  referrer: String
  remoteLoginId: String
}

input WorkflowStepDataInputOpt {
  hypi: HypiInputOpt
  stepName: String
  stepResult: Any
}

input EmailInputOpt {
  hypi: HypiInputOpt
  value: String
  type: String
}

"""Scalar fields defined by Gauge"""
enum GaugeScalarFields {
  """
  A name which uniquely identifies this counter in an instance. Must be a letter followed by 0 or more letters, numbers or underscores
  """
  name

  """A human friendly display label for the counter"""
  label

  """
  The current value of this gauge, set, increase or decrease as you see fit
  """
  value
  tags
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

"""All fields defined by LoginAttempt"""
enum LoginAttemptFields {
  hypi
  successful
  errorCode
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type WorkflowStepData {
  hypi: Hypi
  stepName: String!
  stepResult: Any!
}

"""Scalar fields defined by PermissionDescription"""
enum PermissionDescriptionScalarFields {
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}

input LogMessageInput {
  hypi: HypiInput
  level: LogLevel!
  message: String
  stackTrace: String
  releaseId: String
  type: String
  workflow: String
}

"""All fields defined by OAuthProvider"""
enum OAuthProviderFields {
  hypi

  """ instanceId-(hypi.id = registrationId)"""
  clientId
  clientSecret
  clientAuthenticationMethod
  authorizationGrantType
  redirectUriTemplate
  authorizationUri
  tokenUri
  userInfoUri
  userInfoAuthenticationMethod
  userNameAttributeName
  jwkSetUri
  clientName
  hypiSuccessRedirectUri
  hypiFailureRedirectUri
  scopes
  configurationMetadata
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

type PasswordReminderAggs {
  valid: AggOtherScalar
  code: AggOtherScalar
  from: AggOtherScalar
  subject: AggOtherScalar
  htmlMessage: AggOtherScalar
  plainTextMessage: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
  to_value: AggOtherScalar
  to_type: AggOtherScalar
}

input StorageCounterMaths {
  size: MathInputInt
}

input RealmLinkInputOpt {
  hypi: HypiInputOpt
  name: String
  accounts: [AccountInputOpt]
}

"""All fields defined by WebhookResponse"""
enum WebhookResponseFields {
  hypi
  status
  headers
  body
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

"""Scalar fields defined by AggFloat"""
enum AggFloatScalarFields {
  avg
  count
  max
  min
  sum
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId

  """The value of the aggregated field for each group"""
  groupValues_key

  """The value of the aggregated field for each group"""
  groupValues_value
}

input VideoInputOpt {
  hypi: HypiInputOpt
  name: String
  thumbnails: [ImageInputOpt]
  file: FileInputOpt
  description: String
  location: GeoInputOpt
}

input URLMaths {
  port: MathInputInt
}

type User {
  hypi: Hypi
  account: Account!
  stories(arcql: String, first: Int, after: String, last: Int, before: String): [UserStory!]
}

input PersonNameGroupByOptions {
  """The field by which to to group the matching data"""
  field: PersonNameScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type Message {
  hypi: Hypi
  from: User
  to: User
  in: MessageGroup
  text: String
  replies(arcql: String, first: Int, after: String, last: Int, before: String): [Message!]
}

input PersonNameInputOpt {
  hypi: HypiInputOpt
  title: String
  firstName: String
  lastName: String
  from: DateTime
  to: DateTime
}

input CurrencyGroupByOptions {
  """The field by which to to group the matching data"""
  field: CurrencyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by UserStory"""
enum UserStoryFields {
  hypi
  text
  img
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input MessageInput {
  hypi: HypiInput
  from: UserInput
  to: UserInput
  in: MessageGroupInput
  text: String
  replies: [MessageInput!]
}

"""Scalar fields defined by EmailVerification"""
enum EmailVerificationScalarFields {
  """
  After the link is clicked from the email, redirect the browser to this URL passing a token in the URL i.e. token=jwt.token.here which can be used get the value in the meta field
  """
  redirectTo

  """
  The verification code that is included in the email sent. Generated by the server, if provided the provided value is ignored
  """
  code

  """
  Optionally, the email from which the email will be sent. You MUST have a Hypi email app configured to send from this address
  """
  from

  """
  Optionally, the subject of the email, this is a velocity template - Hypi provides a default such as "Please verify your email to <realm>"
  """
  subject
  templateName

  """
   The HTML contents of the email. This is a Velocity template that will be rendered before being sent.
   The available variables and their types are:
   instance: AppId - You app instance ID
   parent - a map representing the current EmailVerification object
   value - the value of the htmlMessage field
   env: HypiEnv
   
  """
  htmlMessage

  """
  A plain text version of the email - see this is a velocity template, see htmlMessage for available variables
  """
  plainTextMessage

  """
  Any additional meta data you want to store. For example, you could collect all of the information needed to create the Account
  """
  meta

  """Set by system, cannot be provided"""
  confirmed
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  email_value
  email_type
}

input HypiInput {
  id: ID
  impl: String
  created: DateTime
  updated: DateTime
  trashed: DateTime
  createdBy: ID
  instanceId: String
  tags: [ABACTagInput!]
}

type RolePolicyAggs {
  name: AggOtherScalar
  logic: AggOtherScalar
  hypi_id: AggOtherScalar
  hypi_impl: AggOtherScalar
  hypi_created: AggOtherScalar
  hypi_updated: AggOtherScalar
  hypi_trashed: AggOtherScalar
  hypi_createdBy: AggOtherScalar
  hypi_instanceId: AggOtherScalar
}

interface WorkflowAsync {
  hypi: Hypi
  async: Boolean
}

input MessageInputOpt {
  hypi: HypiInputOpt
  from: UserInputOpt
  to: UserInputOpt
  in: MessageGroupInputOpt
  text: String
  replies: [MessageInputOpt]
}

input OAuth2AuthorizedClientInput {
  hypi: HypiInput
  clientRegistrationId: String
  principalName: String
  accessToken: String
  refreshToken: String
}

input AccountPolicyInput {
  hypi: HypiInput
  accounts: [AccountInput!]
  name: String!
  logic: AuthLogic
}

"""All fields defined by Permission"""
enum PermissionFields {
  hypi
  name

  """defines how the policy arrives at a decision, defaults to Unanimous"""
  decisionStrategy

  """The type that this permission applies to"""
  type

  """
   If present then the scopes in this permission will have the given policies applied to this resource.
   This can be used for example to prevent mutation on a resource by a user, group etc
   
  """
  resource

  """Query, Mutation or Subscription"""
  operationType

  """
  If true, this permission grants/denies access to all accounts (including anonymous account)
  """
  includeAllAccounts
  policies
  scopes
  operations
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input WorkflowSessionInputOpt {
  hypi: HypiInputOpt
  data: [WorkflowStepDataInputOpt]
}

input EmailTemplateGroupByOptions {
  """The field by which to to group the matching data"""
  field: EmailTemplateScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

type AccountPolicy implements Policy {
  hypi: Hypi
  name: String!

  """Positive` or `Negative"""
  logic: AuthLogic
  accounts(arcql: String, first: Int, after: String, last: Int, before: String): [Account!]
}

input RequestTemplateGroupByOptions {
  """The field by which to to group the matching data"""
  field: RequestTemplateScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

enum AggOrder {
  ASC
  DESC
}

input RemoteLoginGroupByOptions {
  """The field by which to to group the matching data"""
  field: RemoteLoginScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

"""All fields defined by StorageCounter"""
enum StorageCounterFields {
  hypi
  type
  field
  size
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
}

input RealmPolicyGroupByOptions {
  """The field by which to to group the matching data"""
  field: RealmPolicyScalarFields!

  """
  If provided, the aggregated data will be ordered by this field and any other field which specifies this field, in the order they're defined
  """
  order: AggOrder

  """
  Applies only to DateTime fields. If provided then the date is grouped to this granularity e.g. if MINUTES then the grouping will be per minute
  """
  dateGranularity: TimeUnit
}

input EmailVerificationInput {
  hypi: HypiInput
  email: EmailInput!
  redirectTo: String!
  code: String
  from: String
  subject: String
  templateName: String
  htmlMessage: String
  plainTextMessage: String
  meta: Json
  confirmed: Boolean
}

input AddressInputOpt {
  hypi: HypiInputOpt
  door: String
  street: String
  town: String
  county: String
  city: String
  country: CountryInputOpt
  postCode: String
  from: DateTime
  to: DateTime
}

input WorkflowParallelInput {
  execAs: String
  async: Boolean
  parallel: Boolean
  name: String
  maxExecutionTime: String
  repeatN: Int
  hypi: HypiInput
  steps: [WorkflowStepInput!]
  evaluateIf: GraphQLRefInput
  cronSchedule: String
  repeatIf: GraphQLRefInput
}

input PhoneInput {
  hypi: HypiInput
  number: String!
  country: CountryInput
  code: String
}

"""All fields defined by EmailMessage"""
enum EmailMessageFields {
  hypi
  from
  subject

  """
  Name of the template to use, if present then the given template is used and text/html etc fields in this message are not used
  """
  template
  text
  html

  """
   Schedule sending in the future
   
  """
  deliveryTime

  """
   If set to True or yes this requires the message only be sent over a TLS connection. If a TLS connection can not be established, we will not deliver the message.
   If set to False or no, we will still try and upgrade the connection, if that fails the message will be delivered over a plaintext SMTP connection.
   
  """
  requireTls

  """
   If set to True or yes, the certificate and hostname will not be verified when trying to establish a TLS connection and Hypi will accept any certificate during delivery.
  
   If set to False or no, Hypi will verify the certificate and hostname. If either one can not be verified, a TLS connection will not be established.
  
   The default is False.
   
  """
  skipVerification

  """
   allows to append a custom MIME header to the message (X-My-Header in this case). For example, h:Reply-To to specify Reply-To address.
   
  """
  headers

  """
   prefix followed by an arbitrary name allows to attach a custom JSON data to the message. See Attaching Data to Messages for more information.
   
  """
  variables

  """
   A valid JSON-encoded dictionary, where key is a plain recipient address and value is a dictionary with variables that can be referenced in the message body.
   
  """
  recipientVariables
  to
  cc
  bcc
  attachment
  inline
  tags
  responses
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
  hypi_hypi_hidden_tags
  hypi_tags
  from_hypi
  from_value
  from_type
  to_hypi
  to_value
  to_type
  cc_hypi
  cc_value
  cc_type
  bcc_hypi
  bcc_value
  bcc_type
}

"""Scalar fields defined by Message"""
enum MessageScalarFields {
  text
  hypi_id
  hypi_impl
  hypi_created
  hypi_updated
  hypi_trashed
  hypi_createdBy
  hypi_instanceId
}
